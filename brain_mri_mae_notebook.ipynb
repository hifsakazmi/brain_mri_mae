{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hifsakazmi/brain_mri_mae/blob/main/brain_mri_mae_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "dTkMfw2CggRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRYD4OBjKKVw",
        "outputId": "bec2fbc6-427f-4ed8-f31d-d2239eea3eda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive first\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the latest code from Github"
      ],
      "metadata": {
        "id": "ydnxQgeQgkRj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIHLEf_4KPmp",
        "outputId": "9a707c44-9557-45e2-c431-7c73135809af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'brain_mri_mae'...\n",
            "remote: Enumerating objects: 473, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 473 (delta 3), reused 8 (delta 2), pack-reused 462 (from 1)\u001b[K\n",
            "Receiving objects: 100% (473/473), 76.26 KiB | 5.08 MiB/s, done.\n",
            "Resolving deltas: 100% (279/279), done.\n"
          ]
        }
      ],
      "source": [
        "# CLONE GITHUB REPO\n",
        "!rm -rf mri_mae_project\n",
        "!git clone https://github.com/hifsakazmi/brain_mri_mae.git brain_mri_mae\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Project Dependencies"
      ],
      "metadata": {
        "id": "TmMWpiLigtAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5V3xqiU7GZl",
        "outputId": "da587323-31d1-45b9-f59d-31097bde2112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain_mri_mae\n",
            "Obtaining file:///content/brain_mri_mae\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from brain_mri_mae==0.1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from brain_mri_mae==0.1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from brain_mri_mae==0.1.0) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from brain_mri_mae==0.1.0) (11.3.0)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from brain_mri_mae==0.1.0) (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub->brain_mri_mae==0.1.0) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub->brain_mri_mae==0.1.0) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub->brain_mri_mae==0.1.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub->brain_mri_mae==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->brain_mri_mae==0.1.0) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->brain_mri_mae==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->brain_mri_mae==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->brain_mri_mae==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->brain_mri_mae==0.1.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->brain_mri_mae==0.1.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->brain_mri_mae==0.1.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->brain_mri_mae==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub->brain_mri_mae==0.1.0) (2025.11.12)\n",
            "Installing collected packages: brain_mri_mae\n",
            "  Running setup.py develop for brain_mri_mae\n",
            "Successfully installed brain_mri_mae-0.1.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/brain_mri_mae\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cXs-4zOKTGS",
        "outputId": "eb28df42-de4e-45c6-afd8-5f70d58a0327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# PULL LATEST CHANGES\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FXfHqg5asD-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f70a0fd-0ca4-47f4-eb31-9ef90f127773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain_mri_mae\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('/content/brain_mri_mae')\n",
        "%cd /content/brain_mri_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrain MAE"
      ],
      "metadata": {
        "id": "OEICKmY7Dqpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slpSTRUvKVT6",
        "outputId": "98d7c316-fc31-4778-f234-39108fbb7d50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain_mri_mae\n"
          ]
        }
      ],
      "source": [
        "# RUN MAE PRETRAINING\n",
        "%cd /content/brain_mri_mae\n",
        "!python run_pretrain.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Classifier"
      ],
      "metadata": {
        "id": "ADtXl_-ZD4ut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-DFlu43KYSf",
        "outputId": "83e7b4be-0d7a-48b7-952d-9a41a234f145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[MRIDataset] Loaded 5712 samples from /kaggle/input/brain-tumor-mri-dataset/Training\n",
            "[MRIDataset] Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Train dataset: 4569 samples (80% of training data)\n",
            "[MRIDataset] Loaded 5712 samples from /kaggle/input/brain-tumor-mri-dataset/Training\n",
            "[MRIDataset] Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Validation dataset: 1143 samples (20% of training data)\n",
            "Number of classes: 4\n",
            "Class names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Training samples: 4569\n",
            "Validation samples: 1143\n",
            "âœ… Found latest MAE encoder in Drive: mae_proper_encoder_20251129_114140.pth\n",
            "ðŸ”„ Loading checkpoint from: /content/drive/MyDrive/brain_mri_mae/models/mae_proper_encoder_20251129_114140.pth\n",
            "âœ… Pre-trained weights loaded successfully\n",
            "âœ… Classifier created with pre-trained weights\n",
            "=== MODEL STRUCTURE DEBUG ===\n",
            "Input shape: 224x224\n",
            "Patch embed weight shape: torch.Size([768, 3, 16, 16])\n",
            "=============================\n",
            "Starting fine-tuning...\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.001565\n",
            "cls_token: grad norm = 0.004554\n",
            "patch_embed.weight: grad norm = 0.567033\n",
            "patch_embed.bias: grad norm = 0.020270\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.150656\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.009149\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.195935\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.016427\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.087858\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.003295\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.167703\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.009256\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.009283\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.010111\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.008737\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.009547\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.106348\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.004017\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.141066\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.007366\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.078657\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.002894\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.148938\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.008097\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.008257\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.008876\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.008124\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.008622\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.097516\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.003640\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.131226\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.006645\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.076444\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.002795\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.144436\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.007793\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.007987\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.008499\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.007982\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.008384\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.095503\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.003536\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.131680\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.006488\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.077174\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.002814\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.145801\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.007851\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.008126\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.008587\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.008190\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.008548\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.098864\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.003644\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.137983\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.006649\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.081445\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.002969\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.151742\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.008174\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.008585\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.008988\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.008671\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.008983\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.105851\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.003888\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.147605\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.006984\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.087180\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.003175\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.163565\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.008762\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.009267\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.009617\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.009402\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.009695\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.115715\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.004239\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.161925\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007542\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.095398\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003469\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.180420\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009580\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.010241\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.010510\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.010502\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.010681\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.131676\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.004820\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.179697\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.008263\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.108727\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003951\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.207392\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010842\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011393\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.011627\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.011717\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.011955\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.150250\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.005496\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.207569\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.009407\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.127343\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004618\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.243922\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.012509\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.012997\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.013239\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.013644\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.013862\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.182613\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006680\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.248000\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.011173\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.159234\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.005764\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.303004\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.015095\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.015437\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.015862\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.016465\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.016982\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.240801\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.008813\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.327420\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.014730\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.216481\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.007823\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.420482\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.020494\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.020234\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.020496\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.022641\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.022581\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.370219\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.013558\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.482589\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.022256\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.328648\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.011851\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.663515\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.030878\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.030909\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.029952\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.035364\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.034565\n",
            "classifier.0.weight: grad norm = 0.035484\n",
            "classifier.0.bias: grad norm = 0.034591\n",
            "classifier.2.weight: grad norm = 1.691484\n",
            "classifier.2.bias: grad norm = 0.057410\n",
            "classifier.5.weight: grad norm = 1.481086\n",
            "classifier.5.bias: grad norm = 0.162020\n",
            "======================\n",
            "Batch 0/143, Loss: 1.4023\n",
            "Batch 50/143, Loss: 1.3361\n",
            "Batch 100/143, Loss: 1.2528\n",
            "Epoch 1/30 | Time: 175.16s\n",
            "  Train Loss: 1.1844 | Train Acc: 0.4465 | Train F1: 0.4501\n",
            "  Val Acc: 0.5188 | Val F1: 0.5082 | Val AUC: 0.7908\n",
            "  Val Precision: 0.5230 | Val Recall: 0.5188\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.5188\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.022215\n",
            "cls_token: grad norm = 0.029642\n",
            "patch_embed.weight: grad norm = 1.578399\n",
            "patch_embed.bias: grad norm = 0.059158\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.516806\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.032514\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.587268\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.054111\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.225355\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.009059\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.368451\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.023065\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.023629\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.026175\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.022018\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.024603\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.293041\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.011251\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.293589\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.017126\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.167511\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.006602\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.292573\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.017423\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.018560\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.019859\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.019669\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.019985\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.264387\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.009719\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.258962\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.013302\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.148527\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.005674\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.260329\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.014414\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.017712\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.016878\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.019244\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.017154\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.254826\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.008691\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.243512\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.011002\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.141211\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.005267\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.237893\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.012426\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.017645\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.014979\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.018042\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.014833\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.228866\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.007303\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.234796\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.009643\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.137317\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.005027\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.224682\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.011184\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.017426\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.013753\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.016871\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.013207\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.202197\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.006245\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.229598\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.008825\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.134459\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004810\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.220310\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.010552\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.017202\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.013092\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.016617\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012392\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.188499\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.005892\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.225205\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.008306\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.130942\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004597\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.222577\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.010244\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.017177\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.012691\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.016825\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.012390\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.182499\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005839\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.221791\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.008060\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.128758\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004482\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.228672\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010174\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.017225\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.012579\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.017237\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.012670\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.184005\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.005908\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.221893\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008073\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.129896\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004503\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.238231\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.010410\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.017568\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.012817\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.017952\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.013116\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.192237\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006229\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.230173\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008655\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.138657\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004849\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.262091\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.011383\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.018531\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.013603\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.019266\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.014261\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.211878\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006995\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.258577\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.010334\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.161229\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.005689\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.309993\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.013975\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.020273\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.015919\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.021777\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.017563\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.268505\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.009338\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.347211\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.015388\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.230582\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.008427\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.458082\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.021562\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.025982\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.022798\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.029694\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.026954\n",
            "classifier.0.weight: grad norm = 0.029536\n",
            "classifier.0.bias: grad norm = 0.027048\n",
            "classifier.2.weight: grad norm = 1.120465\n",
            "classifier.2.bias: grad norm = 0.041266\n",
            "classifier.5.weight: grad norm = 2.451351\n",
            "classifier.5.bias: grad norm = 0.259743\n",
            "======================\n",
            "Batch 0/143, Loss: 1.0810\n",
            "Batch 50/143, Loss: 0.9642\n",
            "Batch 100/143, Loss: 0.9839\n",
            "Epoch 2/30 | Time: 180.24s\n",
            "  Train Loss: 1.0239 | Train Acc: 0.5557 | Train F1: 0.5578\n",
            "  Val Acc: 0.5696 | Val F1: 0.5602 | Val AUC: 0.8231\n",
            "  Val Precision: 0.6033 | Val Recall: 0.5696\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.5696\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.039794\n",
            "cls_token: grad norm = 0.016327\n",
            "patch_embed.weight: grad norm = 1.790939\n",
            "patch_embed.bias: grad norm = 0.108764\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.664069\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.054583\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.793735\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.092332\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.283496\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.012722\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.481448\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.030868\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.028509\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.034881\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.029378\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.034173\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.472032\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.020134\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.314719\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.022786\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.192656\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.008216\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.346242\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.020177\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.021986\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.023993\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.023203\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.024323\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.339392\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.013577\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.233381\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.015182\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.154421\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.005865\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.285805\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.013988\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.018128\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.016926\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.020599\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.017908\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.311266\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.009986\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.224645\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.010956\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.139101\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.004639\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.252541\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.010559\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.016047\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.012793\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.018044\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.013455\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.283761\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.007417\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.231530\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.008565\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.129757\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.003989\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.232421\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.009031\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.014154\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.011026\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.015619\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.011673\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.251437\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.005998\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.244568\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.007659\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.126250\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.003806\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.231760\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.008923\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.013541\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.010793\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.014979\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012024\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.244796\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.005912\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.262096\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007764\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.132405\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004020\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.251219\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009687\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.014316\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.011572\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.015500\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.013066\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.239847\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.006188\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.279889\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.008342\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.143460\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004439\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.274308\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010824\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.015460\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.012851\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.016193\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.013808\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.237442\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.006485\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.290407\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008785\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.151227\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004737\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.288546\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.011579\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.016359\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.013845\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.017012\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.014466\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.242874\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006863\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.297165\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.009290\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.159842\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.005107\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.310341\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.012338\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.017455\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.014927\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.018226\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.015554\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.260588\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.007643\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.316178\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.010500\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.181266\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.005867\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.345509\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.014130\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.019016\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.016728\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.019888\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.017698\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.313438\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.009685\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.375536\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.013646\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.243082\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.008016\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.458751\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.019544\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.022553\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.021265\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.025490\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.024294\n",
            "classifier.0.weight: grad norm = 0.024352\n",
            "classifier.0.bias: grad norm = 0.022811\n",
            "classifier.2.weight: grad norm = 0.949910\n",
            "classifier.2.bias: grad norm = 0.031102\n",
            "classifier.5.weight: grad norm = 2.227462\n",
            "classifier.5.bias: grad norm = 0.174596\n",
            "======================\n",
            "Batch 0/143, Loss: 0.9643\n",
            "Batch 50/143, Loss: 1.0683\n",
            "Batch 100/143, Loss: 0.8702\n",
            "Epoch 3/30 | Time: 180.25s\n",
            "  Train Loss: 0.8978 | Train Acc: 0.6325 | Train F1: 0.6311\n",
            "  Val Acc: 0.7052 | Val F1: 0.6937 | Val AUC: 0.8942\n",
            "  Val Precision: 0.7159 | Val Recall: 0.7052\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.7052\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.131531\n",
            "cls_token: grad norm = 0.023195\n",
            "patch_embed.weight: grad norm = 3.091887\n",
            "patch_embed.bias: grad norm = 0.108110\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.897028\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.057800\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.971497\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.097857\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.605688\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.020030\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.950768\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.038288\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.049602\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.041184\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.045474\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.036349\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.065593\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.026804\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.520423\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.024268\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.313901\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.010091\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.491764\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.021148\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.027672\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.023190\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.026903\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.022219\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.671829\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.016585\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.301837\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.013210\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.171705\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.005878\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.259383\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.012652\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.017400\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.014560\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.016687\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.014658\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.331491\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.008897\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.223834\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.009108\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.121022\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.004127\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.182134\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.009328\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.012967\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.010930\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.013075\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.011905\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.204048\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.006395\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.181380\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.007677\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.098192\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.003438\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.154221\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.008391\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.011224\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.009777\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.011717\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.010946\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.162062\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.005731\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.160288\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.007038\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.086556\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.003088\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.141169\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.008023\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.011001\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.009421\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.011521\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.010259\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.138249\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.005155\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.148181\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.006598\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.080852\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.002963\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.137455\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.007940\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.011621\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.009449\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.011848\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.009870\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.125549\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.004768\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.145653\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.006472\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.078978\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.002997\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.140297\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.008069\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.012092\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.009690\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.012318\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.009943\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.123337\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004721\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.147627\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006492\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.082396\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003197\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.150212\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.008324\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.012356\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.010123\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.012635\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.010338\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.130831\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004982\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.158003\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.006872\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.093049\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003630\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.176237\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.009143\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012947\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.010807\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.013663\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.011095\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.148260\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005529\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.178616\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.007716\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.111111\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.004245\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.214560\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.010546\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.013655\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.011822\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.014952\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.012565\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.190210\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007131\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.233448\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.010227\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.158295\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.005768\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.313082\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.014164\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.016444\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.014699\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.019022\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.016955\n",
            "classifier.0.weight: grad norm = 0.020483\n",
            "classifier.0.bias: grad norm = 0.014973\n",
            "classifier.2.weight: grad norm = 0.614806\n",
            "classifier.2.bias: grad norm = 0.020130\n",
            "classifier.5.weight: grad norm = 2.176036\n",
            "classifier.5.bias: grad norm = 0.121277\n",
            "======================\n",
            "Batch 0/143, Loss: 1.0120\n",
            "Batch 50/143, Loss: 0.6061\n",
            "Batch 100/143, Loss: 0.5378\n",
            "Epoch 4/30 | Time: 180.33s\n",
            "  Train Loss: 0.7584 | Train Acc: 0.7137 | Train F1: 0.7123\n",
            "  Val Acc: 0.7690 | Val F1: 0.7722 | Val AUC: 0.9310\n",
            "  Val Precision: 0.7880 | Val Recall: 0.7690\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.7690\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.077512\n",
            "cls_token: grad norm = 0.023947\n",
            "patch_embed.weight: grad norm = 4.693949\n",
            "patch_embed.bias: grad norm = 0.284120\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.465328\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.160891\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.510397\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.245037\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.722423\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.030890\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.200609\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.075741\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.080636\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.083999\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.086258\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.086886\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.237953\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.054455\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.935545\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.062689\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.527060\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.023742\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.845464\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.059386\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.063659\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.068557\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.070126\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.073644\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.105944\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.046787\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.632940\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.047160\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.380690\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.018023\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.555523\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.043211\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.045858\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.055120\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.047051\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.054997\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.697232\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.032327\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.465762\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.033430\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.270864\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.012716\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.391725\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.030907\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.032019\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.040196\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.032560\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.039428\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.469944\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.022247\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.361924\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.024669\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.204632\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.009571\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.300358\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.023793\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.023465\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.030736\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.023586\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.030129\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.345447\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.016511\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.290243\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.018562\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.153769\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.007287\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.228986\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.018494\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.018083\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.023879\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.017646\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.022762\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.253282\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.011947\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.237677\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.013974\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.122119\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.005725\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.186137\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.014711\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.014831\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.018957\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.014319\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.017721\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.192972\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.008983\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.204933\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.011187\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.104710\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004785\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.169225\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.012749\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.012898\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.016140\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.012572\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.015247\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.162193\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.007371\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.186784\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.009593\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.096423\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004287\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.165930\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.011707\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011842\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.014556\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.011864\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.014135\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.153423\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006827\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.182070\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008960\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.095943\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004158\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.175933\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.011379\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.011722\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.013984\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.011995\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.013980\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.162414\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006976\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.193127\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.009261\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.108609\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.004476\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.200276\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.011828\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.012774\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.014279\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.013277\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.014608\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.193498\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007957\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.231249\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.010787\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.144031\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.005636\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.273474\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.014435\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.015866\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.016566\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.017593\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.018242\n",
            "classifier.0.weight: grad norm = 0.017826\n",
            "classifier.0.bias: grad norm = 0.018468\n",
            "classifier.2.weight: grad norm = 0.506598\n",
            "classifier.2.bias: grad norm = 0.018129\n",
            "classifier.5.weight: grad norm = 1.676646\n",
            "classifier.5.bias: grad norm = 0.080792\n",
            "======================\n",
            "Batch 0/143, Loss: 0.5208\n",
            "Batch 50/143, Loss: 0.6471\n",
            "Batch 100/143, Loss: 0.4488\n",
            "Epoch 5/30 | Time: 180.63s\n",
            "  Train Loss: 0.6185 | Train Acc: 0.7779 | Train F1: 0.7774\n",
            "  Val Acc: 0.8276 | Val F1: 0.8260 | Val AUC: 0.9516\n",
            "  Val Precision: 0.8291 | Val Recall: 0.8276\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8276\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.084945\n",
            "cls_token: grad norm = 0.021433\n",
            "patch_embed.weight: grad norm = 4.329294\n",
            "patch_embed.bias: grad norm = 0.200336\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.153163\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.101905\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.248351\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.158752\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.502684\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.022499\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.834337\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.055777\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.051015\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.063310\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.049323\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.061351\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.900090\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.034009\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.595353\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.040490\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.328014\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.013813\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.575304\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.037518\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.032563\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.041987\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.034314\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.043600\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.691021\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.025778\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.437995\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.027710\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.231746\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.009720\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.393123\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.027565\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.023527\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.031406\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.024602\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.033118\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.470202\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.020928\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.301501\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.019269\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.171404\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.006954\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.282058\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.019010\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.017822\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.022103\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.018662\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.023390\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.353102\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.014691\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.236993\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.014145\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.129633\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.005076\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.204788\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.013380\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.013937\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.016098\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.014419\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.016583\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.246501\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.009278\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.187334\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.010162\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.106280\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004092\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.159176\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.010193\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.011647\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.012466\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.012205\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012436\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.179415\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.006298\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.164343\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007669\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.087026\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003253\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.131258\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.008050\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.010996\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.010094\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.011062\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.009803\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.139184\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.004733\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.150794\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.006130\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.080796\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.002854\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.127423\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.007024\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.010975\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.008742\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.010836\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.008364\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.125368\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004001\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.145962\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.005262\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.079562\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002712\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.131194\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.006579\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011014\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.008022\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010889\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.007774\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.123962\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.003752\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.146153\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.005018\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.082155\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002737\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.143090\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.006599\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.011289\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.007892\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.011459\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.007891\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.133883\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.003968\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.156667\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.005546\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.094915\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003141\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.170060\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.007497\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.011915\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.008522\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.012276\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.008955\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.158415\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.004873\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.189457\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.007285\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.130783\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004408\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.231366\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.010012\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.013616\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.010865\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.014510\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.012657\n",
            "classifier.0.weight: grad norm = 0.013848\n",
            "classifier.0.bias: grad norm = 0.011961\n",
            "classifier.2.weight: grad norm = 0.457481\n",
            "classifier.2.bias: grad norm = 0.015006\n",
            "classifier.5.weight: grad norm = 1.805914\n",
            "classifier.5.bias: grad norm = 0.081166\n",
            "======================\n",
            "Batch 0/143, Loss: 0.4536\n",
            "Batch 50/143, Loss: 0.4888\n",
            "Batch 100/143, Loss: 0.8888\n",
            "Epoch 6/30 | Time: 180.51s\n",
            "  Train Loss: 0.5295 | Train Acc: 0.8153 | Train F1: 0.8147\n",
            "  Val Acc: 0.8014 | Val F1: 0.7933 | Val AUC: 0.9485\n",
            "  Val Precision: 0.8179 | Val Recall: 0.8014\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.087226\n",
            "cls_token: grad norm = 0.049389\n",
            "patch_embed.weight: grad norm = 8.540346\n",
            "patch_embed.bias: grad norm = 0.313775\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.263582\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.172481\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.232967\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.244668\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.874384\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.037333\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.355353\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.099813\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.093694\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.116276\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.091997\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.107388\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.404353\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.061111\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.027946\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.072491\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.584656\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.024294\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.918028\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.066105\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.065937\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.075316\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.069463\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.074822\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.133569\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.047932\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.726802\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.049316\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.433122\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.018502\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.624762\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.046289\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.048092\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.057339\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.049112\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.055640\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.809955\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.035265\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.522688\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.033322\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.310587\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.013116\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.431419\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.030995\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.034693\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.040466\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.034695\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.038542\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.559708\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.023245\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.377011\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.023710\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.225337\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.009353\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.308567\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.021959\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.025402\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.029290\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.024576\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.027326\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.400483\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.015702\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.282161\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.017169\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.160451\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.006705\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.218651\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.016060\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.017996\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.021547\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.016700\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.019543\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.268240\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.010347\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.218318\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.012193\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.120222\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004992\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.164682\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.012244\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.013637\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.016352\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012316\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.014816\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.184783\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.007407\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.181212\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.009357\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.099273\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004045\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.145559\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010408\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011177\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.013443\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.010307\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.012518\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.149274\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.006052\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.164337\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.007876\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.088624\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003563\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.142527\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.009606\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.009906\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.011991\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.009481\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.011560\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.138437\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.005556\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.159289\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.007288\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.086536\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003421\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.152627\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.009365\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.009560\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.011498\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.009730\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.011531\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.145154\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005700\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.164831\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.007470\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.093731\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003612\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.173300\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.009763\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010000\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.011779\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010459\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.012213\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.174430\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.006674\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.197333\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.008822\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.123995\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004657\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.230632\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.011709\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011413\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.013794\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.012657\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.015519\n",
            "classifier.0.weight: grad norm = 0.012309\n",
            "classifier.0.bias: grad norm = 0.015614\n",
            "classifier.2.weight: grad norm = 0.403444\n",
            "classifier.2.bias: grad norm = 0.014575\n",
            "classifier.5.weight: grad norm = 2.117053\n",
            "classifier.5.bias: grad norm = 0.109201\n",
            "======================\n",
            "Batch 0/143, Loss: 0.7422\n",
            "Batch 50/143, Loss: 0.4843\n",
            "Batch 100/143, Loss: 0.5152\n",
            "Epoch 7/30 | Time: 180.13s\n",
            "  Train Loss: 0.4404 | Train Acc: 0.8475 | Train F1: 0.8471\n",
            "  Val Acc: 0.8285 | Val F1: 0.8294 | Val AUC: 0.9587\n",
            "  Val Precision: 0.8408 | Val Recall: 0.8285\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8285\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.114796\n",
            "cls_token: grad norm = 0.043044\n",
            "patch_embed.weight: grad norm = 8.588201\n",
            "patch_embed.bias: grad norm = 0.308769\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.184537\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.191439\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.140710\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.264238\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.013793\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.038201\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.595406\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.097814\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.105911\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.111674\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.103144\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.103747\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.667862\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.065677\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.221348\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.075481\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.699735\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.027182\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.052946\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.068533\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.077026\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.079776\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.078482\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.078411\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.391935\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.053581\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.776164\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.054520\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.461816\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.020686\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.637936\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.048400\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.053213\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.062929\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.051265\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.058538\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.816540\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.036136\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.548349\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.036458\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.316976\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.014548\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.418236\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.032356\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.037371\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.044303\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.034328\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.039348\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.514265\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.022656\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.380752\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.024431\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.224548\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.009979\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.287232\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.021273\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.026382\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.030212\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.022969\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.024676\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.341422\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.013712\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.260895\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.015490\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.150383\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.006516\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.193702\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.013828\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.017801\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.019704\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.015343\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.015558\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.213531\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.008080\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.195522\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.009809\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.108856\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004420\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.144663\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009514\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.013173\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.013299\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012068\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.010857\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.150646\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005555\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.157345\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.007044\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.089295\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003409\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.127216\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.007641\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011484\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.010244\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.010856\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.008941\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.121516\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004308\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.140580\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.005731\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.080673\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002947\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.126384\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.006782\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.010856\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.008798\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010686\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.008176\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.116613\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.003974\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.137431\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.005366\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.081343\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002889\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.137555\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.006829\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.010776\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.008302\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.010796\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.008130\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.121379\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.004042\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.142354\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.005602\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.087755\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003020\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.153581\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.007172\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010969\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.008469\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010903\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.008973\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.146879\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.005061\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.170962\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.007217\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.115364\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004009\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.215062\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.009685\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011895\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.010195\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.012841\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.011615\n",
            "classifier.0.weight: grad norm = 0.012468\n",
            "classifier.0.bias: grad norm = 0.012235\n",
            "classifier.2.weight: grad norm = 0.357764\n",
            "classifier.2.bias: grad norm = 0.011846\n",
            "classifier.5.weight: grad norm = 1.135323\n",
            "classifier.5.bias: grad norm = 0.036259\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3511\n",
            "Batch 50/143, Loss: 0.4853\n",
            "Batch 100/143, Loss: 0.3335\n",
            "Epoch 8/30 | Time: 180.64s\n",
            "  Train Loss: 0.3892 | Train Acc: 0.8689 | Train F1: 0.8687\n",
            "  Val Acc: 0.8530 | Val F1: 0.8519 | Val AUC: 0.9633\n",
            "  Val Precision: 0.8578 | Val Recall: 0.8530\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8530\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.098622\n",
            "cls_token: grad norm = 0.038055\n",
            "patch_embed.weight: grad norm = 8.698542\n",
            "patch_embed.bias: grad norm = 0.291067\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.245235\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.178811\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.174865\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.249871\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.969672\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.039246\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.454314\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.096477\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.098774\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.114002\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.093018\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.103395\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.413833\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.057353\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.996385\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.069834\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.597008\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.024018\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.941337\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.061145\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.064501\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.071020\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.067604\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.068289\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.143000\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.042908\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.632059\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.045580\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.387046\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.016352\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.570079\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.041018\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.042668\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.050297\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.042341\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.048094\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.714809\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.030030\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.430142\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.029880\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.258115\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.011102\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.364835\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.026970\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.029290\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.034718\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.028334\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.032408\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.466912\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.019485\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.323728\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.020267\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.185531\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.007980\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.261388\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.018599\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.020944\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.024690\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.019768\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.022167\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.332085\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.012753\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.254874\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.013980\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.136384\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.005805\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.195638\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.013054\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.015363\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.017563\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.014018\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.015167\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.228999\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.008068\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.207713\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.009424\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.107327\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004299\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.159531\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009604\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.012249\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.012771\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.011264\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.011254\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.170846\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005637\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.182372\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.007151\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.091935\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003467\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.144349\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.008133\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.010776\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.010372\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.010216\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.009712\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.146369\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004644\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.169008\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006160\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.083740\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003074\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.141478\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.007770\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.010199\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.009413\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010016\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.009483\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.139605\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004537\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.165194\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.006101\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.083147\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003084\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.152837\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.008164\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.010175\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.009477\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.010319\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.009958\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.145129\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.004935\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.169890\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.006625\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.090632\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003373\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.169849\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.008677\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010446\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.010018\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010914\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.010679\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.166092\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.005739\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.192674\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.007948\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.115816\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004189\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.212762\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.010284\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011872\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.011714\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.012748\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.012795\n",
            "classifier.0.weight: grad norm = 0.015312\n",
            "classifier.0.bias: grad norm = 0.014500\n",
            "classifier.2.weight: grad norm = 0.375574\n",
            "classifier.2.bias: grad norm = 0.013085\n",
            "classifier.5.weight: grad norm = 1.581948\n",
            "classifier.5.bias: grad norm = 0.063513\n",
            "======================\n",
            "Batch 0/143, Loss: 0.6606\n",
            "Batch 50/143, Loss: 0.3214\n",
            "Batch 100/143, Loss: 0.1918\n",
            "Epoch 9/30 | Time: 180.53s\n",
            "  Train Loss: 0.3475 | Train Acc: 0.8816 | Train F1: 0.8814\n",
            "  Val Acc: 0.8591 | Val F1: 0.8602 | Val AUC: 0.9662\n",
            "  Val Precision: 0.8629 | Val Recall: 0.8591\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8591\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.129308\n",
            "cls_token: grad norm = 0.043090\n",
            "patch_embed.weight: grad norm = 5.290502\n",
            "patch_embed.bias: grad norm = 0.196576\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.631776\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.115575\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.842873\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.174837\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.770332\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.024310\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.279410\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.056076\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.074862\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.057854\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.075466\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.056270\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.221521\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.034472\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.881447\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.044492\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.537442\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.017839\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.891923\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.041865\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.054096\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.044768\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.055991\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.045479\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.142143\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.030188\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.630966\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.031455\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.366656\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.012582\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.558932\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.029729\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.037868\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.033842\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.037702\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.034670\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.644961\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.021698\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.447972\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.023778\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.266584\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.009375\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.387440\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.022701\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.029300\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.026504\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.028824\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.026827\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.445463\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.015990\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.306285\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.017609\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.187187\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.006902\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.270409\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.017104\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.020967\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.020421\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.020759\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.020596\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.317467\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.011910\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.227451\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.012852\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.129088\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.005061\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.187434\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.012555\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.014745\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.015543\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.014308\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.015046\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.210181\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.007998\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.181014\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.009292\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.100983\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003972\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.145302\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009957\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.011911\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.012446\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.011208\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.011661\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.146189\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005596\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.154993\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.007327\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.084971\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003219\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.125513\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.008478\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.010580\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.010544\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.009992\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.010018\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.125689\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004741\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.143544\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006305\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.078541\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002892\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.122014\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.007841\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.009874\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.009619\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.009452\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.009274\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.120254\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004445\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.139278\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.005933\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.079061\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002868\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.131673\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.007748\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.009731\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.009294\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.009643\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.009251\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.128261\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.004675\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.144940\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.006186\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.086929\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003095\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.152063\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.008134\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010149\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.009549\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010308\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.009928\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.151678\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.005409\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.171397\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.007513\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.116781\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004214\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.211597\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.010501\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011740\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.011346\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.012983\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.012609\n",
            "classifier.0.weight: grad norm = 0.013035\n",
            "classifier.0.bias: grad norm = 0.011138\n",
            "classifier.2.weight: grad norm = 0.392708\n",
            "classifier.2.bias: grad norm = 0.013825\n",
            "classifier.5.weight: grad norm = 1.282735\n",
            "classifier.5.bias: grad norm = 0.045979\n",
            "======================\n",
            "Batch 0/143, Loss: 0.2803\n",
            "Batch 50/143, Loss: 0.2708\n",
            "Batch 100/143, Loss: 0.2112\n",
            "Epoch 10/30 | Time: 180.29s\n",
            "  Train Loss: 0.3241 | Train Acc: 0.8930 | Train F1: 0.8930\n",
            "  Val Acc: 0.8723 | Val F1: 0.8720 | Val AUC: 0.9680\n",
            "  Val Precision: 0.8737 | Val Recall: 0.8723\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8723\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.098229\n",
            "cls_token: grad norm = 0.041155\n",
            "patch_embed.weight: grad norm = 6.097933\n",
            "patch_embed.bias: grad norm = 0.277817\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.897136\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.153222\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.149290\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.248202\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.709854\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.030984\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.209553\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.091089\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.073612\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.098039\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.075991\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.101219\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.219835\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.062495\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.937461\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.076978\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.525721\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.025040\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.920256\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.075290\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.054463\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.081704\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.060093\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.088128\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.136674\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.060144\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.720511\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.063986\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.388333\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.021277\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.650720\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.060687\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.039242\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.072426\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.040503\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.071699\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.786306\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.044534\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.443923\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.045876\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.265496\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.015616\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.441047\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.042580\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.027347\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.052462\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.025928\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.050833\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.465737\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.028392\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.322678\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.031293\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.190145\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.010987\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.323194\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.030110\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.019684\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.037415\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.017746\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.035845\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.301268\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.018481\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.256564\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.021995\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.151969\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.008086\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.260102\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.022197\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.014660\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.027802\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.013899\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.026378\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.237634\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.012994\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.231065\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.016308\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.130600\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.006291\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.215653\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.017438\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.012771\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.021743\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012526\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.020770\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.196374\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.009757\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.209643\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.012798\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.113656\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.005129\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.186260\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.014675\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011892\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.018060\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.011471\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.017360\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.173325\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.008082\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.196657\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.010747\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.106690\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004583\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.177919\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.013156\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011246\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.016056\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010871\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.015658\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.165482\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.007387\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.189404\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.009797\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.105961\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004399\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.189992\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.012503\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.010861\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.015109\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.011049\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.015146\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.170466\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.007453\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.192767\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.009973\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.112424\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.004492\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.214029\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.012718\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.011124\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.014893\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.011723\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.015574\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.192831\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.008262\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.215531\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.011252\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.137961\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.005358\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.270878\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.014706\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.012894\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.016348\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.014873\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.018255\n",
            "classifier.0.weight: grad norm = 0.016654\n",
            "classifier.0.bias: grad norm = 0.020181\n",
            "classifier.2.weight: grad norm = 0.487858\n",
            "classifier.2.bias: grad norm = 0.017831\n",
            "classifier.5.weight: grad norm = 2.464190\n",
            "classifier.5.bias: grad norm = 0.110636\n",
            "======================\n",
            "Batch 0/143, Loss: 0.4207\n",
            "Batch 50/143, Loss: 0.4137\n",
            "Batch 100/143, Loss: 0.5000\n",
            "Epoch 11/30 | Time: 180.22s\n",
            "  Train Loss: 0.5230 | Train Acc: 0.8172 | Train F1: 0.8164\n",
            "  Val Acc: 0.8320 | Val F1: 0.8307 | Val AUC: 0.9580\n",
            "  Val Precision: 0.8336 | Val Recall: 0.8320\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.063493\n",
            "cls_token: grad norm = 0.018476\n",
            "patch_embed.weight: grad norm = 3.053760\n",
            "patch_embed.bias: grad norm = 0.131552\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.855945\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.088258\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.870613\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.120294\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.435962\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.016589\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.728720\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.042611\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.044026\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.047813\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.044580\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.047374\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.786085\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.029260\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.522619\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.032960\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.308929\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.011790\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.509056\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.030459\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.031062\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.034560\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.034557\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.035101\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.617391\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.024437\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.385516\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.025364\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.231835\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.009365\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.356485\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.024015\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.024002\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.028635\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.025716\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.029100\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.474127\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.019822\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.280457\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.018594\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.165545\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.006817\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.242051\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.017563\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.017495\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.021778\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.017294\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.021835\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.298673\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.013211\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.210643\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.013517\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.118533\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.005056\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.177026\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.013124\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.012942\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.016544\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.012844\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.016390\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.209840\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.008827\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.178658\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.010135\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.095787\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004014\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.145627\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.010529\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.011090\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.013438\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.011218\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012968\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.158732\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.006417\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.164408\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.008094\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.087802\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003434\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.133245\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009177\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.011013\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.011581\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.010759\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.010887\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.132240\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005164\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.151356\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.006864\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.081700\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003052\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.127564\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.008441\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.010812\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.010416\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.010474\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.009933\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.123142\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004615\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.145769\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006317\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.079743\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002988\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.129475\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.008287\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.010559\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.010054\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010345\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.009913\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.123630\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004658\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.145216\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.006335\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.082486\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003153\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.140550\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.008604\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.010587\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.010297\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.010626\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.010420\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.131249\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005069\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.152614\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.006861\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.091094\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003478\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.161729\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.009143\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010762\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.010958\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.011000\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.011535\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.160330\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.006181\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.185523\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.008422\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.120171\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004455\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.225316\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.011505\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011600\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.013216\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.012307\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.015183\n",
            "classifier.0.weight: grad norm = 0.012175\n",
            "classifier.0.bias: grad norm = 0.013699\n",
            "classifier.2.weight: grad norm = 0.403035\n",
            "classifier.2.bias: grad norm = 0.013994\n",
            "classifier.5.weight: grad norm = 1.853433\n",
            "classifier.5.bias: grad norm = 0.061150\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3872\n",
            "Batch 50/143, Loss: 0.2549\n",
            "Batch 100/143, Loss: 0.4279\n",
            "Epoch 12/30 | Time: 180.30s\n",
            "  Train Loss: 0.4699 | Train Acc: 0.8376 | Train F1: 0.8373\n",
            "  Val Acc: 0.8425 | Val F1: 0.8391 | Val AUC: 0.9552\n",
            "  Val Precision: 0.8400 | Val Recall: 0.8425\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.050512\n",
            "cls_token: grad norm = 0.011722\n",
            "patch_embed.weight: grad norm = 2.613259\n",
            "patch_embed.bias: grad norm = 0.117628\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.824073\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.081263\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.863414\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.111757\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.414349\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.022751\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.617932\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.048157\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.038390\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.055312\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.037195\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.054079\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.655740\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.033307\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.391003\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.034609\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.234842\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.012576\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.371238\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.030918\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.021863\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.036200\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.022839\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.037764\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.469425\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.022639\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.288989\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.022897\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.152662\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.007858\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.235120\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.020790\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.015471\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.025663\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.015287\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.025064\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.284637\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.013231\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.189843\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.015716\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.108435\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.005397\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.164391\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.014543\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.011717\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.018151\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.011450\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.017615\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.179600\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.009088\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.149914\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.011694\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.081849\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.004114\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.129517\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.011557\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.009524\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.014339\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.009366\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.014233\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.133622\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.007100\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.124186\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.009542\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.068867\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.003481\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.114297\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.009965\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.008369\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.012471\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.008217\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012232\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.107949\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.005777\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.116125\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007968\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.061532\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.002981\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.105791\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.008824\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.008078\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.010909\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.007966\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.010550\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.096309\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.004890\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.112337\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.006659\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.057995\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.002593\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.101491\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.007946\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.007983\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.009591\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.008119\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.009320\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.094175\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004381\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.111130\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.005852\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.058614\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002411\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.103473\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.007349\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.008199\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.008759\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.008602\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.008617\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.097654\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004139\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.117198\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.005408\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.063953\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002406\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.115106\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.007037\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.009008\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.008335\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.009684\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.008415\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.109878\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.004128\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.133272\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.005705\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.076572\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.002823\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.140164\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.007507\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010376\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.008621\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.011127\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.008774\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.133545\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.004693\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.162052\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.006745\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.108023\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.003912\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.198358\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.009423\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.012089\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.010077\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.013629\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.011188\n",
            "classifier.0.weight: grad norm = 0.014864\n",
            "classifier.0.bias: grad norm = 0.012018\n",
            "classifier.2.weight: grad norm = 0.393668\n",
            "classifier.2.bias: grad norm = 0.012836\n",
            "classifier.5.weight: grad norm = 1.748666\n",
            "classifier.5.bias: grad norm = 0.066393\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3450\n",
            "Batch 50/143, Loss: 0.3647\n",
            "Batch 100/143, Loss: 0.4559\n",
            "Epoch 13/30 | Time: 180.30s\n",
            "  Train Loss: 0.4363 | Train Acc: 0.8534 | Train F1: 0.8534\n",
            "  Val Acc: 0.7795 | Val F1: 0.7857 | Val AUC: 0.9519\n",
            "  Val Precision: 0.8154 | Val Recall: 0.7795\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.158779\n",
            "cls_token: grad norm = 0.013679\n",
            "patch_embed.weight: grad norm = 4.375502\n",
            "patch_embed.bias: grad norm = 0.210173\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.307440\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.153483\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.425298\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.219367\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.839232\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.038365\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.297102\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.079138\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.073907\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.087730\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.071921\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.083715\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.557275\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.061769\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.919592\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.059543\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.525378\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.023348\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.817103\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.054862\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.048183\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.063459\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.049021\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.064237\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.123877\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.046477\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.649789\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.039636\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.356549\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.015382\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.519685\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.034354\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.034446\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.045522\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.033388\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.042022\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.709861\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.027293\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.418033\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.024203\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.245208\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.010570\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.353175\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.020300\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.025401\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.028367\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.024916\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.025473\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.419447\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.015936\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.320545\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.014866\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.179277\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.007066\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.263691\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.013697\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.020628\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.018692\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.020683\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.018347\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.307652\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.010661\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.281320\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.010900\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.154151\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.005589\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.222537\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.011647\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.019293\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.015119\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.019247\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.015013\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.250604\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.008034\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.273466\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.009391\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.145227\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004954\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.209639\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.010938\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.020267\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.013767\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.020320\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.013532\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.224511\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.006697\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.265268\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.008669\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.132110\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004427\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.200395\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010625\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.020994\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.012933\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.020989\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.012610\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.220113\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.006060\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.261437\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008201\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.128708\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004308\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.205737\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.010657\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.021760\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.012772\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.021855\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.012419\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.226354\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.005976\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.262995\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008207\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.134288\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004418\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.226826\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.010796\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.022717\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.013015\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.022795\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.012793\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.242557\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006369\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.268722\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.008882\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.154378\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.005048\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.258336\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.011655\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.023312\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.013913\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.023152\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.014000\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.273773\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007466\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.296458\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.010740\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.193763\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.006242\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.332402\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.014786\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.024500\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.016323\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.026314\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.018947\n",
            "classifier.0.weight: grad norm = 0.027879\n",
            "classifier.0.bias: grad norm = 0.019906\n",
            "classifier.2.weight: grad norm = 0.641435\n",
            "classifier.2.bias: grad norm = 0.021105\n",
            "classifier.5.weight: grad norm = 3.237895\n",
            "classifier.5.bias: grad norm = 0.126015\n",
            "======================\n",
            "Batch 0/143, Loss: 0.7275\n",
            "Batch 50/143, Loss: 0.4078\n",
            "Batch 100/143, Loss: 0.2780\n",
            "Epoch 14/30 | Time: 180.14s\n",
            "  Train Loss: 0.3665 | Train Acc: 0.8748 | Train F1: 0.8747\n",
            "  Val Acc: 0.8731 | Val F1: 0.8740 | Val AUC: 0.9692\n",
            "  Val Precision: 0.8789 | Val Recall: 0.8731\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8731\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.066199\n",
            "cls_token: grad norm = 0.034733\n",
            "patch_embed.weight: grad norm = 4.762767\n",
            "patch_embed.bias: grad norm = 0.189815\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.846021\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.153944\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.010508\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.209020\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.559319\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.027553\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.812704\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.068085\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.057163\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.079151\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.054204\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.074184\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.885233\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.043325\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.646996\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.049275\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.359509\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.016619\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.581502\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.044750\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.037652\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.051513\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.038691\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.052718\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.647574\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.030962\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.496737\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.035044\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.299590\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.012505\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.441840\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.032593\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.030584\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.039990\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.030257\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.039608\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.550307\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.023171\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.350897\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.024064\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.206021\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.009055\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.292257\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.022218\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.022450\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.029372\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.020776\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.026642\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.379078\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.015114\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.251306\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.015918\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.144208\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.006264\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.196939\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.014864\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.016260\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.020089\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.014495\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.017751\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.226215\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.009235\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.214761\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.010847\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.119105\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004739\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.166683\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.011101\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.013480\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.014854\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.012476\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.013135\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.173374\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.006341\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.202970\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.008255\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.108451\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003942\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.158949\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.009518\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.012577\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.012392\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.011939\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.011287\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.163648\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.005104\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.200700\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.007097\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.098419\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003326\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.149883\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.008955\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011889\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.011054\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.011467\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.010397\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.167196\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004768\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.200601\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006609\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.094349\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003011\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.147879\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.008757\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011578\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.010466\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.011466\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.010163\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.172205\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004710\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.202951\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.006531\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.097979\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002992\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.159508\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.009105\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012156\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.010464\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.012314\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.010601\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.185559\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005124\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.207683\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.007005\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.111122\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003388\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.184903\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.009845\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.013183\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.010968\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.013370\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.011588\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.208735\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.006050\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.222620\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.008731\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.138411\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004417\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.229425\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.011783\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.015070\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.012947\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.015867\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.014716\n",
            "classifier.0.weight: grad norm = 0.016391\n",
            "classifier.0.bias: grad norm = 0.017941\n",
            "classifier.2.weight: grad norm = 0.411566\n",
            "classifier.2.bias: grad norm = 0.014837\n",
            "classifier.5.weight: grad norm = 2.636811\n",
            "classifier.5.bias: grad norm = 0.108492\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3004\n",
            "Batch 50/143, Loss: 0.5858\n",
            "Batch 100/143, Loss: 0.3254\n",
            "Epoch 15/30 | Time: 180.51s\n",
            "  Train Loss: 0.3434 | Train Acc: 0.8792 | Train F1: 0.8797\n",
            "  Val Acc: 0.8338 | Val F1: 0.8338 | Val AUC: 0.9693\n",
            "  Val Precision: 0.8572 | Val Recall: 0.8338\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.121381\n",
            "cls_token: grad norm = 0.030258\n",
            "patch_embed.weight: grad norm = 7.788074\n",
            "patch_embed.bias: grad norm = 0.376506\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.884052\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.277149\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 3.005543\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.375087\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.959571\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.045822\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.384014\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.116123\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.098876\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.136628\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.095709\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.132192\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.695155\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.077703\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.979432\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.082433\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.590929\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.027330\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.898905\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.074574\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.063020\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.086770\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.066056\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.087529\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.154815\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.054190\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.791819\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.056783\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.449065\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.020429\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.617555\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.052853\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.046538\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.065612\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.046170\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.063504\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.845329\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.041262\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.557153\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.039740\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.322641\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.015336\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.416899\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.036328\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.032384\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.048871\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.030843\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.044303\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.572559\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.026761\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.376375\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.026582\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.212804\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.010434\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.261851\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.024169\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.022445\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.033769\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.020164\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.029611\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.334033\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.015889\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.264627\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.017825\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.150508\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.007316\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.180001\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.017071\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.016824\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.023896\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.014690\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.020505\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.209868\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.009973\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.204603\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.012556\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.117651\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.005432\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.144954\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.013446\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.013794\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.018056\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012220\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.015999\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.159079\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.007401\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.181260\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.009998\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.096057\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004276\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.130478\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.011945\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.012078\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.015038\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.011201\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.013973\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.144243\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.006417\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.169446\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008713\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.084709\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003644\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.129319\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.011276\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011161\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.013597\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.010768\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.013201\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.143349\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006163\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.168346\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008269\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.082114\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003416\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.139834\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.011081\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.010962\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.013071\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.010919\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.013136\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.150432\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006279\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.171001\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.008345\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.089466\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003618\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.164963\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.011198\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010870\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.013231\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.011244\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.013820\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.176464\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007106\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.198631\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.009489\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.118356\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004602\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.220747\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.012764\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.012287\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.015196\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.013521\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.016971\n",
            "classifier.0.weight: grad norm = 0.015862\n",
            "classifier.0.bias: grad norm = 0.018603\n",
            "classifier.2.weight: grad norm = 0.396616\n",
            "classifier.2.bias: grad norm = 0.015209\n",
            "classifier.5.weight: grad norm = 2.866879\n",
            "classifier.5.bias: grad norm = 0.125288\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3352\n",
            "Batch 50/143, Loss: 0.3123\n",
            "Batch 100/143, Loss: 0.2103\n",
            "Epoch 16/30 | Time: 180.36s\n",
            "  Train Loss: 0.3305 | Train Acc: 0.8851 | Train F1: 0.8852\n",
            "  Val Acc: 0.8171 | Val F1: 0.8128 | Val AUC: 0.9544\n",
            "  Val Precision: 0.8348 | Val Recall: 0.8171\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.125896\n",
            "cls_token: grad norm = 0.057191\n",
            "patch_embed.weight: grad norm = 8.415335\n",
            "patch_embed.bias: grad norm = 0.306643\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.390808\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.201363\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.184477\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.260768\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.003278\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.040761\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.452983\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.094970\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.109619\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.112577\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.106892\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.097260\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.551116\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.061500\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.050891\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.070736\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.722541\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.027366\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.078248\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.066026\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.080083\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.073287\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.084436\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.075727\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.286600\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.055917\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.827432\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.055683\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.519773\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.020971\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.719543\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.051441\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.057349\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.062976\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.057481\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.062414\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.981374\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.044115\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.563128\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.040261\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.342981\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.014837\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.453211\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.035713\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.037594\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.047064\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.036771\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.044277\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.673591\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.028698\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.382462\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.027024\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.230671\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.010289\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.292231\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.023742\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.023999\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.032939\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.021959\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.029240\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.405157\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.016895\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.272929\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.017966\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.154898\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.007110\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.201995\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.016419\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.015089\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.022934\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.013370\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.019606\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.236310\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.010125\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.205716\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.012042\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.116633\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.005176\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.163102\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.012120\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.011377\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.016591\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.010426\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.014549\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.168494\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.007126\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.173694\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.009158\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.093313\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003917\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.142695\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010340\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.009984\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.013261\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.009728\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.012343\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.143187\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.005799\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.160730\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.007791\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.079905\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003181\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.137586\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.009688\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.009422\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.011660\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.009754\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.011619\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.140178\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.005442\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.160289\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.007424\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.079602\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003016\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.149665\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.009959\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.009434\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.011417\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.010199\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.012082\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.155235\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005886\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.169400\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.007957\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.088844\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003217\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.174031\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.010538\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.009679\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.011942\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010912\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.013305\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.184545\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007028\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.202607\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.009663\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.121550\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004463\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.236851\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.012727\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.011953\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.014493\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.013853\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.016991\n",
            "classifier.0.weight: grad norm = 0.018414\n",
            "classifier.0.bias: grad norm = 0.020543\n",
            "classifier.2.weight: grad norm = 0.362732\n",
            "classifier.2.bias: grad norm = 0.013823\n",
            "classifier.5.weight: grad norm = 1.869522\n",
            "classifier.5.bias: grad norm = 0.084056\n",
            "======================\n",
            "Batch 0/143, Loss: 0.4796\n",
            "Batch 50/143, Loss: 0.2858\n",
            "Batch 100/143, Loss: 0.2696\n",
            "Epoch 17/30 | Time: 180.52s\n",
            "  Train Loss: 0.2899 | Train Acc: 0.9017 | Train F1: 0.9016\n",
            "  Val Acc: 0.8058 | Val F1: 0.8059 | Val AUC: 0.9608\n",
            "  Val Precision: 0.8265 | Val Recall: 0.8058\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.169607\n",
            "cls_token: grad norm = 0.021416\n",
            "patch_embed.weight: grad norm = 6.137406\n",
            "patch_embed.bias: grad norm = 0.346151\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.946562\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.212000\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.325664\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.329619\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.157019\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.058237\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.851608\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.136533\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.117714\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.142507\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.119633\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.156728\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 2.217069\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.126203\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.597509\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.121263\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.905090\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.044934\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.449009\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.119240\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.096073\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.130361\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.114696\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.152544\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 2.458997\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.123523\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 1.154507\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.101705\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.616069\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.034892\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.892879\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.087020\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.065546\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.115109\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.064657\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.107388\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 1.254897\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.070000\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.653140\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.063394\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.407184\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.022877\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.571143\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.050291\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.042017\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.073788\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.035552\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.061829\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.684798\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.036155\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.477049\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.037385\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.288511\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.014284\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.387977\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.031101\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.026605\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.046405\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.021867\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.037653\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.426881\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.020041\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.362865\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.023525\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.215689\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.009652\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.285069\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.021964\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.018884\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.031994\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.016683\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.027456\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.299712\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.013468\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.282801\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.017384\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.167822\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.007066\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.220767\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.018169\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.015115\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.024647\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.014468\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.022801\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.235288\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.010721\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.243045\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.014479\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.133508\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.005510\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.181128\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.017019\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.013655\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.021079\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.013334\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.020203\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.201452\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.009442\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.224346\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.012862\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.116080\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004752\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.172866\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.016544\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.012949\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.019527\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.013040\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.019249\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.195009\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.009123\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.219803\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.012179\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.113226\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004586\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.194704\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.016292\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012738\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.019057\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.013610\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.019321\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.207473\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.009487\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.229801\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.012550\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.122630\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.004936\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.240776\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.016408\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.013496\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.019326\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.015473\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.020539\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.242171\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.010716\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.271907\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.014262\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.166196\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.006544\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.330117\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.018709\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.016973\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.021908\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.020901\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.024774\n",
            "classifier.0.weight: grad norm = 0.021620\n",
            "classifier.0.bias: grad norm = 0.024291\n",
            "classifier.2.weight: grad norm = 0.522624\n",
            "classifier.2.bias: grad norm = 0.019276\n",
            "classifier.5.weight: grad norm = 2.798533\n",
            "classifier.5.bias: grad norm = 0.113462\n",
            "======================\n",
            "Batch 0/143, Loss: 0.2392\n",
            "Batch 50/143, Loss: 0.1737\n",
            "Batch 100/143, Loss: 0.1456\n",
            "Epoch 18/30 | Time: 180.11s\n",
            "  Train Loss: 0.2493 | Train Acc: 0.9177 | Train F1: 0.9177\n",
            "  Val Acc: 0.8618 | Val F1: 0.8645 | Val AUC: 0.9689\n",
            "  Val Precision: 0.8704 | Val Recall: 0.8618\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.205132\n",
            "cls_token: grad norm = 0.032731\n",
            "patch_embed.weight: grad norm = 8.014279\n",
            "patch_embed.bias: grad norm = 0.336884\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.513968\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.208592\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.470530\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.298428\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.113134\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.040731\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.687161\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.094129\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.112974\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.106470\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.116145\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.100132\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.903654\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.071096\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.210198\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.075755\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.774323\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.028631\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.191473\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.071912\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.083661\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.078193\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.093740\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.085994\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.623391\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.065758\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.909889\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.059856\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.552999\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.022271\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.760304\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.053019\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.058032\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.066175\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.059407\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.064020\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 1.051868\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.042503\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.603424\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.039565\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.383509\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.015437\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.509343\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.035776\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.041802\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.047397\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.040106\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.042539\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.664956\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.025590\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.417116\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.025864\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.258819\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.010595\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.327818\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.024217\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.028339\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.032887\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.025408\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.027691\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.446338\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.016349\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.279103\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.017178\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.165085\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.007154\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.193541\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.016203\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.018695\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.022612\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.015816\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.018635\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.246599\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.010013\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.209762\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.011917\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.126380\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.005540\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.144175\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.012198\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.014398\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.017050\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012141\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.014212\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.162447\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.007047\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.176046\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.009359\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.099917\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004369\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.127206\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.010903\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.012588\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.014114\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.011463\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.012671\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.137235\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.005954\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.162193\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008274\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.085718\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003726\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.125501\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.010484\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.011921\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.012849\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.011495\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.012359\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.135262\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.005821\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.162891\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008099\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.084803\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003654\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.142916\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.010854\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012184\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.012874\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.012264\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.013025\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.147959\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006304\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.170213\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.008793\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.091534\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003844\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.174903\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.011583\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.012523\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.013610\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.013078\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.014422\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.180610\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007619\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.207562\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.010744\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.124182\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.005068\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.246046\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.013977\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.014516\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.016014\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.015809\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.018157\n",
            "classifier.0.weight: grad norm = 0.016818\n",
            "classifier.0.bias: grad norm = 0.018909\n",
            "classifier.2.weight: grad norm = 0.350131\n",
            "classifier.2.bias: grad norm = 0.013741\n",
            "classifier.5.weight: grad norm = 1.901348\n",
            "classifier.5.bias: grad norm = 0.075753\n",
            "======================\n",
            "Batch 0/143, Loss: 0.2577\n",
            "Batch 50/143, Loss: 0.6235\n",
            "Batch 100/143, Loss: 0.1175\n",
            "Epoch 19/30 | Time: 177.89s\n",
            "  Train Loss: 0.2219 | Train Acc: 0.9286 | Train F1: 0.9287\n",
            "  Val Acc: 0.8854 | Val F1: 0.8846 | Val AUC: 0.9747\n",
            "  Val Precision: 0.8856 | Val Recall: 0.8854\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8854\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.192290\n",
            "cls_token: grad norm = 0.027233\n",
            "patch_embed.weight: grad norm = 6.707115\n",
            "patch_embed.bias: grad norm = 0.246314\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.962096\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.165019\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.130090\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.222052\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.940596\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.036141\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.377120\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.074756\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.088551\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.084203\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.083227\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.080438\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.679145\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.061023\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.054846\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.061258\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.600923\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.023796\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.894496\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.051131\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.062194\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.062057\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.061216\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.057504\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.165481\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.040838\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.789219\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.043053\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.433542\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.016833\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.641692\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.037986\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.045094\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.048720\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.043951\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.043786\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.844052\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.026951\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.563040\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.029381\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.312413\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.012204\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.450224\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.026541\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.031407\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.034933\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.030749\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.032039\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.534672\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.018595\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.423133\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.021087\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.240127\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.009310\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.320094\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.019350\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.024400\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.026227\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.022524\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.023822\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.345788\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.012627\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.312389\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.015693\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.179007\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.006850\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.228870\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.015136\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.019037\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.020215\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.016912\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.018036\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.232386\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.008784\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.237480\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.011709\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.130997\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004986\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.174100\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.012950\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.015552\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.016043\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.014542\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.015184\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.184815\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.007121\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.200149\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.009598\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.098631\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003702\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.141378\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.011817\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.013280\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.013703\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.013022\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.013735\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.161181\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.006426\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.179189\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.008615\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.085285\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.003198\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.130708\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.011585\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.012465\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.012959\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.012485\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.013197\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.153525\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006310\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.172279\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008318\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.083589\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003212\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.136041\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.011638\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012559\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.012922\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.012671\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.013359\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.158887\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.006741\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.175177\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.008900\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.093861\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003686\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.161211\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.012150\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.013401\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.013485\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.013957\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.014025\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.179515\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.007673\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.197126\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.010348\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.124366\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.004945\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.220607\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.013601\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.015080\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.015203\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.016325\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.016600\n",
            "classifier.0.weight: grad norm = 0.014475\n",
            "classifier.0.bias: grad norm = 0.014905\n",
            "classifier.2.weight: grad norm = 0.328855\n",
            "classifier.2.bias: grad norm = 0.010921\n",
            "classifier.5.weight: grad norm = 1.482307\n",
            "classifier.5.bias: grad norm = 0.039402\n",
            "======================\n",
            "Batch 0/143, Loss: 0.2839\n",
            "Batch 50/143, Loss: 0.4385\n",
            "Batch 100/143, Loss: 0.0664\n",
            "Epoch 20/30 | Time: 177.78s\n",
            "  Train Loss: 0.1993 | Train Acc: 0.9372 | Train F1: 0.9372\n",
            "  Val Acc: 0.8985 | Val F1: 0.8991 | Val AUC: 0.9791\n",
            "  Val Precision: 0.9014 | Val Recall: 0.8985\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.8985\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.064026\n",
            "cls_token: grad norm = 0.030144\n",
            "patch_embed.weight: grad norm = 1.958725\n",
            "patch_embed.bias: grad norm = 0.080760\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.708869\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.066780\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.744113\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.095445\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.306450\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.011443\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.451264\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.027562\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.028318\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.030582\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.026930\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.030442\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.518718\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.020754\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.365007\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.022478\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.213309\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.008193\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.321001\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.020454\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.019298\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.023086\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.019551\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.023678\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.422924\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.015495\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.282429\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.016465\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.165423\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.006189\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.222347\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.015475\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.015381\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.018709\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.014178\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.017703\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.277133\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.011392\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.191125\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.011572\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.115552\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.004574\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.155235\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.011431\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.010864\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.014323\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.010261\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.013039\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.208148\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.008181\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.132920\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.008101\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.072517\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.002912\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.095122\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.007746\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.007707\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.009898\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.007167\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.009160\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.113963\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.005018\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.098842\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.005604\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.051178\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.002028\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.063203\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.005691\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.006038\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.007121\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.005600\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.006492\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.076531\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.003215\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.079025\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.003977\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.041093\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.001555\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.052420\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.004586\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.005213\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.005587\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.005026\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.005164\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.062360\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.002388\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.071696\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.003203\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.034348\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.001269\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.048774\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.004106\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.004903\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.004777\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.004791\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.004577\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.058319\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.002107\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.069660\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.002889\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.031890\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.001156\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.050116\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.003935\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.004831\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.004434\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.004778\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.004407\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.058904\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.002054\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.070801\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.002832\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.032683\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.001173\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.055553\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.004034\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.004991\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.004482\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.004964\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.004543\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.064436\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.002228\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.074310\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.003101\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.037296\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.001346\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.067178\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.004309\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.005178\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.004721\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.005357\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.005027\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.076971\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.002671\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.088347\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.003771\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.053547\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.001920\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.097697\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.005334\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.005931\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.005727\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.006334\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.006536\n",
            "classifier.0.weight: grad norm = 0.007390\n",
            "classifier.0.bias: grad norm = 0.006725\n",
            "classifier.2.weight: grad norm = 0.185815\n",
            "classifier.2.bias: grad norm = 0.006666\n",
            "classifier.5.weight: grad norm = 1.409766\n",
            "classifier.5.bias: grad norm = 0.045465\n",
            "======================\n",
            "Batch 0/143, Loss: 0.1041\n",
            "Batch 50/143, Loss: 0.1625\n",
            "Batch 100/143, Loss: 0.0856\n",
            "Epoch 21/30 | Time: 180.66s\n",
            "  Train Loss: 0.1550 | Train Acc: 0.9534 | Train F1: 0.9534\n",
            "  Val Acc: 0.8635 | Val F1: 0.8616 | Val AUC: 0.9735\n",
            "  Val Precision: 0.8702 | Val Recall: 0.8635\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.213577\n",
            "cls_token: grad norm = 0.049444\n",
            "patch_embed.weight: grad norm = 8.025964\n",
            "patch_embed.bias: grad norm = 0.262944\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.174033\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.177426\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.103620\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.242189\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.236355\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.039055\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.840576\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.081901\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.126320\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.090692\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.127035\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.084405\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 2.197322\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.064456\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.366676\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.069147\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.873714\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.028104\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.299080\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.067434\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.094357\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.071240\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.098964\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.075029\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.740559\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.061698\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 1.020154\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.056619\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.617864\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.022365\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.835367\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.053083\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.064386\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.064275\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.064638\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.060878\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 1.135774\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.043973\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.627942\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.040331\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.413361\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.015934\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.524571\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.034823\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.042446\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.047060\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.040152\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.041724\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.726067\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.025794\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.420299\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.026224\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.264821\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.010152\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.348702\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.024302\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.028001\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.032250\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.025865\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.029620\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.438432\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.016845\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.319378\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.019069\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.175821\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.007131\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.233469\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.018489\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.019617\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.024554\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.017439\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.022006\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.274235\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.011050\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.258115\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.014468\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.134233\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.005603\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.184536\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.015908\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.016214\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.020146\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.014834\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.018470\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.203142\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.008624\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.234677\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.012099\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.113045\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.004693\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.167193\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.014900\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.015324\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.017899\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.014565\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.016994\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.188322\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.007828\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.223592\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.011038\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.103195\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004248\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.166527\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.014287\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.015035\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.016833\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.014751\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.016388\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.188088\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.007613\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.221119\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.010589\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.103861\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.004194\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.179431\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.014037\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.015399\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.016545\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.015463\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.016396\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.200302\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.007859\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.227369\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.010894\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.116537\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.004576\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.213548\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.014427\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.015925\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.016873\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.016324\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.017237\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.233692\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.009015\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.260891\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.012424\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.152448\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.005746\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.304940\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.016451\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.017356\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.018807\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.018774\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.021296\n",
            "classifier.0.weight: grad norm = 0.018932\n",
            "classifier.0.bias: grad norm = 0.020786\n",
            "classifier.2.weight: grad norm = 0.431980\n",
            "classifier.2.bias: grad norm = 0.015508\n",
            "classifier.5.weight: grad norm = 1.512045\n",
            "classifier.5.bias: grad norm = 0.064619\n",
            "======================\n",
            "Batch 0/143, Loss: 0.3335\n",
            "Batch 50/143, Loss: 0.1703\n",
            "Batch 100/143, Loss: 0.0720\n",
            "Epoch 22/30 | Time: 180.21s\n",
            "  Train Loss: 0.1436 | Train Acc: 0.9567 | Train F1: 0.9567\n",
            "  Val Acc: 0.9046 | Val F1: 0.9048 | Val AUC: 0.9794\n",
            "  Val Precision: 0.9056 | Val Recall: 0.9046\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.9046\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.190592\n",
            "cls_token: grad norm = 0.014622\n",
            "patch_embed.weight: grad norm = 3.868080\n",
            "patch_embed.bias: grad norm = 0.171917\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 1.178092\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.115910\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.281732\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.169252\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.613313\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.021568\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.934148\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.052403\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.055805\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.055174\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.055186\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.053685\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.027639\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.037520\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.756343\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.043307\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.446315\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.015633\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.710248\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.041411\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.041546\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.043618\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.042163\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.045038\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.830381\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.029842\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.600338\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.033829\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.380331\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.013195\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.567816\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.034206\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.034517\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.038057\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.034984\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.038197\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.759528\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.027024\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.378009\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.025283\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.233535\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.009316\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.350434\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.024098\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.022058\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.028844\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.021690\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.028628\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.467578\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.019165\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.241899\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.017242\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.144512\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.006235\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.206851\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.015948\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.015377\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.020478\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.015056\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.019566\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.270075\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.011433\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.179353\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.011646\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.093557\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004391\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.120296\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.010893\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.010695\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.014515\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.009503\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012972\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.153954\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.006717\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.131460\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007814\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.069499\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.003181\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.086051\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.008281\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.007855\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.010834\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.006893\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.009692\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.097113\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.004503\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.108564\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.006012\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.054286\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.002392\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.071595\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.007010\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.006528\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.008701\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.005944\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.008082\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.084084\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.003727\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.097071\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.005060\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.046340\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.001964\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.067204\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.006451\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.005794\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.007656\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.005448\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.007381\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.079165\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.003426\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.091610\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.004627\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.044368\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.001816\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.069062\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.006226\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.005533\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.007230\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.005338\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.007145\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.080342\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.003421\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.091387\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.004613\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.046548\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.001837\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.079863\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.006152\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.005318\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.007060\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.005516\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.007287\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.092011\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.003816\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.100509\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.005153\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.063013\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.002387\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.109283\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.006976\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.005872\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.007720\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.006317\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.008373\n",
            "classifier.0.weight: grad norm = 0.007627\n",
            "classifier.0.bias: grad norm = 0.009395\n",
            "classifier.2.weight: grad norm = 0.188522\n",
            "classifier.2.bias: grad norm = 0.006750\n",
            "classifier.5.weight: grad norm = 1.760617\n",
            "classifier.5.bias: grad norm = 0.058260\n",
            "======================\n",
            "Batch 0/143, Loss: 0.1231\n",
            "Batch 50/143, Loss: 0.0334\n",
            "Batch 100/143, Loss: 0.0419\n",
            "Epoch 23/30 | Time: 180.93s\n",
            "  Train Loss: 0.1107 | Train Acc: 0.9650 | Train F1: 0.9650\n",
            "  Val Acc: 0.9011 | Val F1: 0.9017 | Val AUC: 0.9816\n",
            "  Val Precision: 0.9030 | Val Recall: 0.9011\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.249158\n",
            "cls_token: grad norm = 0.029514\n",
            "patch_embed.weight: grad norm = 7.413033\n",
            "patch_embed.bias: grad norm = 0.356674\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.604150\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.263673\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 3.108231\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.383276\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.335550\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.053839\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 1.968343\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.125902\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.126215\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.145059\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.122433\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.142709\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 2.166701\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.093062\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.494964\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.096714\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.857058\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.036367\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.333146\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.089170\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.079702\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.100838\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.079559\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.109946\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.520220\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.068181\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 1.085059\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.071983\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.676739\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.026716\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 1.004627\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.064557\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.063251\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.077896\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.062887\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.078944\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 1.304896\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.051038\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.790879\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.050191\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.502139\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.020374\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.706762\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.046901\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.049483\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.060507\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.045756\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.054792\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.884266\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.035415\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.509911\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.032638\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.322593\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.013338\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.411741\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.029209\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.034120\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.039483\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.030102\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.034248\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.497136\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.020146\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.327726\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.019835\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.193914\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.008202\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.232891\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.017914\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.022011\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.024611\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.018756\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.020542\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.275406\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.010752\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.208444\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.011840\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.122113\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.004986\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.141068\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.011680\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.014738\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.015541\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.012499\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.013182\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.150904\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.006049\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.161201\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.008164\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.085840\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.003361\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.108924\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.009170\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.011258\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.011507\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.009955\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.010370\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.126751\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.004758\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.140853\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.006469\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.067625\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.002558\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.095355\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.008145\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.009346\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.009682\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.008686\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.009131\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.118862\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.004316\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.132260\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.005745\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.062495\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.002290\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.097376\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.007704\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.008549\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.008866\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.008240\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.008681\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.121209\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.004308\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.127909\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.005606\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.066827\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.002406\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.111928\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.007552\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.008077\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.008554\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.008197\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.008647\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.128111\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.004527\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.134064\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.006119\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.087666\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.003091\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.148393\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.008391\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.008504\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.009407\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.009154\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.010402\n",
            "classifier.0.weight: grad norm = 0.010501\n",
            "classifier.0.bias: grad norm = 0.011838\n",
            "classifier.2.weight: grad norm = 0.209767\n",
            "classifier.2.bias: grad norm = 0.007594\n",
            "classifier.5.weight: grad norm = 1.613119\n",
            "classifier.5.bias: grad norm = 0.058219\n",
            "======================\n",
            "Batch 0/143, Loss: 0.1490\n",
            "Batch 50/143, Loss: 0.0175\n",
            "Batch 100/143, Loss: 0.0128\n",
            "Epoch 24/30 | Time: 180.42s\n",
            "  Train Loss: 0.0982 | Train Acc: 0.9731 | Train F1: 0.9731\n",
            "  Val Acc: 0.9125 | Val F1: 0.9123 | Val AUC: 0.9833\n",
            "  Val Precision: 0.9125 | Val Recall: 0.9125\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.9125\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.026861\n",
            "cls_token: grad norm = 0.004402\n",
            "patch_embed.weight: grad norm = 0.774140\n",
            "patch_embed.bias: grad norm = 0.030142\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.271023\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.020621\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.283654\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.029217\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.121392\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.004176\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.182902\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.010027\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.011916\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.010477\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.011476\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.010590\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.221370\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.007496\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.144765\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.008174\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.082895\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.002920\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.129954\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.007689\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.008209\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.008329\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.008399\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.008520\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.165244\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.005684\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.103292\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.006190\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.062659\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.002286\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.093065\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.006109\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.006622\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.006829\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.006490\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.006709\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.119112\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.004328\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.078494\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.004564\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.044637\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.001664\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.066522\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.004523\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.004900\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.005188\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.004812\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.005075\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.091258\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.003131\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.057421\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.003170\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.030625\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.001181\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.046274\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.003295\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.003571\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.003757\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.003456\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.003773\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.055519\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.002009\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.045032\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.002307\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.022709\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.000861\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.035255\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.002551\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.002893\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.002891\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.002896\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.002947\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.039292\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.001444\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.038306\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.001810\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.018775\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.000687\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.029848\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.002171\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.002752\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.002449\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.002815\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.002488\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.031681\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.001169\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.035354\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.001566\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.016375\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.000585\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.027802\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.001999\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.002810\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.002224\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.002885\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.002296\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.029630\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.001070\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.034062\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.001448\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.015458\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.000554\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.027285\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.001968\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.002917\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.002172\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.002994\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.002249\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.029218\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.001052\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.033721\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.001414\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.015695\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.000570\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.027705\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.001963\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.003078\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.002199\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.003161\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.002259\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.030888\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.001108\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.034277\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.001485\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.017906\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.000664\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.030016\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.001966\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.003249\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.002270\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.003304\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.002296\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.034412\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.001225\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.036838\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.001650\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.023450\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.000877\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.035977\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.002057\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.003491\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.002430\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.003548\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.002487\n",
            "classifier.0.weight: grad norm = 0.004244\n",
            "classifier.0.bias: grad norm = 0.002150\n",
            "classifier.2.weight: grad norm = 0.073766\n",
            "classifier.2.bias: grad norm = 0.002662\n",
            "classifier.5.weight: grad norm = 0.542740\n",
            "classifier.5.bias: grad norm = 0.012308\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0252\n",
            "Batch 50/143, Loss: 0.0576\n",
            "Batch 100/143, Loss: 0.1514\n",
            "Epoch 25/30 | Time: 178.20s\n",
            "  Train Loss: 0.0698 | Train Acc: 0.9823 | Train F1: 0.9823\n",
            "  Val Acc: 0.9029 | Val F1: 0.9027 | Val AUC: 0.9837\n",
            "  Val Precision: 0.9085 | Val Recall: 0.9029\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.129283\n",
            "cls_token: grad norm = 0.010475\n",
            "patch_embed.weight: grad norm = 3.576612\n",
            "patch_embed.bias: grad norm = 0.177932\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.982851\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.112666\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 1.202317\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.171947\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.558797\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.020924\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.893596\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.054309\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.055773\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.058159\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.054323\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.056481\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 1.045851\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.041431\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.663891\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.047897\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.391047\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.015847\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.650482\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.044613\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.040337\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.048574\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.042536\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.050163\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.803362\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.035533\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.550258\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.040293\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.319915\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.013743\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.495279\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.037740\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.033079\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.043361\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.033436\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.043671\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.670081\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.031395\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.378966\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.029309\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.217690\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.010074\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.336211\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.026295\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.022864\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.032805\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.022810\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.031638\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.464048\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.020613\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.226583\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.019191\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.130096\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.006474\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.188507\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.016377\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.014858\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.021481\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.013496\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.019742\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.230051\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.011463\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.153645\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.011576\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.084414\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.004089\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.109223\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.010196\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.010067\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.013575\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.009082\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.012091\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.135564\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.006250\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.116407\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.007190\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.065629\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.002860\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.074423\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.007090\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.007904\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.009350\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.007012\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.008226\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.086560\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.003871\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.090741\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.005003\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.049217\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.002014\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.056493\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.005545\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.006644\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.006933\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.005941\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.006222\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.069544\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.002890\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.077442\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.003838\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.039420\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.001549\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.046810\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.004683\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.005597\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.005646\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.005120\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.005207\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.060691\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.002456\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.067931\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.003254\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.034755\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.001333\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.045070\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.004216\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.004965\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.004976\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.004585\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.004699\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.056536\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.002252\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.062803\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.003013\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.035007\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.001322\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.051094\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.004028\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.004347\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.004681\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.004097\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.004593\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.062428\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.002446\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.066080\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.003217\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.041946\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.001558\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.068172\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.004199\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.004104\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.004793\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.004271\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.005100\n",
            "classifier.0.weight: grad norm = 0.004656\n",
            "classifier.0.bias: grad norm = 0.005083\n",
            "classifier.2.weight: grad norm = 0.100717\n",
            "classifier.2.bias: grad norm = 0.003657\n",
            "classifier.5.weight: grad norm = 0.737650\n",
            "classifier.5.bias: grad norm = 0.020431\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0239\n",
            "Batch 50/143, Loss: 0.0086\n",
            "Batch 100/143, Loss: 0.2212\n",
            "Epoch 26/30 | Time: 179.09s\n",
            "  Train Loss: 0.0704 | Train Acc: 0.9825 | Train F1: 0.9825\n",
            "  Val Acc: 0.9230 | Val F1: 0.9229 | Val AUC: 0.9842\n",
            "  Val Precision: 0.9230 | Val Recall: 0.9230\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.9230\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.005056\n",
            "cls_token: grad norm = 0.001422\n",
            "patch_embed.weight: grad norm = 0.150861\n",
            "patch_embed.bias: grad norm = 0.005778\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.041398\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.003473\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.043795\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.005339\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.020420\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.000732\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.030458\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.001794\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.001976\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.002002\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.001837\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.001858\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.035045\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.001155\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.024960\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.001320\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.014460\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.000503\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.022263\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.001185\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.001385\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.001297\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.001388\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.001344\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.027404\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.000839\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.019377\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.000949\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.011776\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.000396\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.017260\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.000905\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.001166\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.001038\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.001142\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.000989\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.021081\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.000667\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.013398\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.000682\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.008760\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.000296\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.012706\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.000677\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.000891\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.000814\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.000903\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.000770\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.015622\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.000489\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.011167\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.000508\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.006573\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.000233\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.009467\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.000494\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.000687\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.000633\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.000688\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.000572\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.011102\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.000329\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.009643\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.000371\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.005719\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.000197\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.007785\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.000412\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.000623\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.000523\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.000590\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.000502\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.008048\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.000252\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.008814\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.000325\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.005172\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.000167\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.007150\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.000384\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.000597\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.000478\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.000563\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.000470\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.007056\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.000213\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.008557\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.000298\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.004514\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.000142\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.006523\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.000375\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.000563\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.000452\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.000546\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.000443\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.006923\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.000203\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.008461\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.000285\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.004145\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.000130\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.006262\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.000374\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.000567\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.000439\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.000567\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.000430\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.007102\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.000204\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.008468\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.000280\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.004197\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.000134\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.006564\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.000382\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.000606\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.000440\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.000617\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.000436\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.007600\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.000216\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.008674\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.000298\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.004687\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.000150\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.007469\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.000396\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.000667\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.000460\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.000694\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.000476\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.008747\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.000260\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.009423\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.000381\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.006128\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.000205\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.010055\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.000515\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.000773\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.000567\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.000847\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.000652\n",
            "classifier.0.weight: grad norm = 0.001933\n",
            "classifier.0.bias: grad norm = 0.000977\n",
            "classifier.2.weight: grad norm = 0.022822\n",
            "classifier.2.bias: grad norm = 0.000916\n",
            "classifier.5.weight: grad norm = 0.163946\n",
            "classifier.5.bias: grad norm = 0.003870\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0071\n",
            "Batch 50/143, Loss: 0.0720\n",
            "Batch 100/143, Loss: 0.0042\n",
            "Epoch 27/30 | Time: 180.49s\n",
            "  Train Loss: 0.0546 | Train Acc: 0.9858 | Train F1: 0.9858\n",
            "  Val Acc: 0.9256 | Val F1: 0.9255 | Val AUC: 0.9843\n",
            "  Val Precision: 0.9254 | Val Recall: 0.9256\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.9256\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.042796\n",
            "cls_token: grad norm = 0.004196\n",
            "patch_embed.weight: grad norm = 1.345827\n",
            "patch_embed.bias: grad norm = 0.089339\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.445012\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.067186\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.473319\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.093256\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.228757\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.008940\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.326900\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.020881\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.021994\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.023644\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.022793\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.023356\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.393248\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.016286\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.288927\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.017210\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.168646\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.006891\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.233835\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.016939\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.017703\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.018829\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.019490\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.020460\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.385948\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.015867\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.202791\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.013095\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.104678\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.004657\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.136894\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.011268\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.011670\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.014829\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.010701\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.012604\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.207403\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.007936\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.115222\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.007403\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.064935\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.002887\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.086118\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.005854\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.006519\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.008626\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.005327\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.006683\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.096778\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.003485\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.071763\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.004013\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.042850\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.001751\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.064815\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.003473\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.004231\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.004985\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.003760\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.004365\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.065269\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.002230\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.060601\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.002804\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.035466\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.001330\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.058377\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.002668\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.003577\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.003706\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.003359\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.003527\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.049862\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.001710\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.058064\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.002335\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.034080\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.001229\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.056898\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.002506\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.003517\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.003394\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.003356\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.003316\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.046373\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.001587\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.056577\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.002222\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.031309\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.001118\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.054595\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.002658\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.003582\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.003345\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.003545\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.003339\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.045852\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.001554\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.056086\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.002206\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.028843\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.001027\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.050705\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.002861\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.003777\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.003409\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.003795\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.003404\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.047380\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.001607\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.055733\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.002265\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.028489\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.001018\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.050454\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.002941\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.003951\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.003489\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.003830\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.003481\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.049626\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.001694\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.056328\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.002369\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.032297\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.001143\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.056953\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.003116\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.003946\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.003631\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.003838\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.003620\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.054821\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.001900\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.058661\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.002727\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.037351\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.001299\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.072234\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.003336\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.004017\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.003788\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.004144\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.003959\n",
            "classifier.0.weight: grad norm = 0.004688\n",
            "classifier.0.bias: grad norm = 0.004195\n",
            "classifier.2.weight: grad norm = 0.093250\n",
            "classifier.2.bias: grad norm = 0.003305\n",
            "classifier.5.weight: grad norm = 0.436451\n",
            "classifier.5.bias: grad norm = 0.010901\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0153\n",
            "Batch 50/143, Loss: 0.0155\n",
            "Batch 100/143, Loss: 0.0958\n",
            "Epoch 28/30 | Time: 180.62s\n",
            "  Train Loss: 0.0469 | Train Acc: 0.9891 | Train F1: 0.9891\n",
            "  Val Acc: 0.9230 | Val F1: 0.9231 | Val AUC: 0.9848\n",
            "  Val Precision: 0.9255 | Val Recall: 0.9230\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.001374\n",
            "cls_token: grad norm = 0.000726\n",
            "patch_embed.weight: grad norm = 0.051919\n",
            "patch_embed.bias: grad norm = 0.001975\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 0.011771\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.001050\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 0.013502\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.001583\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 0.005767\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.000208\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 0.009105\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.000598\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.000592\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.000622\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.000564\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.000612\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 0.010255\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.000368\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 0.008138\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.000465\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.004316\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.000154\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 0.006887\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.000442\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.000445\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.000476\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.000442\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.000490\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 0.008307\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.000318\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 0.006059\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.000370\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.003467\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.000135\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 0.005177\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.000366\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.000356\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.000414\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.000358\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.000413\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 0.006639\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.000262\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.004260\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.000285\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.002486\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.000103\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.003683\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.000296\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.000273\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.000340\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.000270\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.000347\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.004877\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.000201\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.003376\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.000228\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.001983\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.000087\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.003024\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.000243\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.000224\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.000288\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.000227\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.000292\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.003634\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.000162\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.003010\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.000191\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.001850\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.000081\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.003022\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.000209\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.000211\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.000253\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.000214\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.000266\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.002940\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.000134\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.003029\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.000173\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.001833\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.000076\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.003132\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.000204\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.000217\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.000245\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.000218\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.000263\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.002687\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.000121\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.003131\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.000166\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.001716\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.000067\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.002992\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.000213\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.000227\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.000247\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.000229\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.000263\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.002707\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.000121\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.003235\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.000166\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.001614\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.000063\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.002880\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.000223\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.000242\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.000254\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.000251\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.000265\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.002850\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.000124\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.003367\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.000168\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.001704\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.000068\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.003101\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.000228\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.000268\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.000263\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.000287\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.000272\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.003157\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.000133\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.003531\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.000178\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.002029\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.000081\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.003622\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.000236\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.000302\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.000276\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.000338\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.000287\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.003823\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.000155\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.004046\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.000211\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.002636\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.000107\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.004610\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.000273\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.000367\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.000313\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.000411\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.000344\n",
            "classifier.0.weight: grad norm = 0.000975\n",
            "classifier.0.bias: grad norm = 0.000476\n",
            "classifier.2.weight: grad norm = 0.009679\n",
            "classifier.2.bias: grad norm = 0.000428\n",
            "classifier.5.weight: grad norm = 0.075568\n",
            "classifier.5.bias: grad norm = 0.002102\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0032\n",
            "Batch 50/143, Loss: 0.0042\n",
            "Batch 100/143, Loss: 0.2297\n",
            "Epoch 29/30 | Time: 180.28s\n",
            "  Train Loss: 0.0382 | Train Acc: 0.9926 | Train F1: 0.9926\n",
            "  Val Acc: 0.9265 | Val F1: 0.9264 | Val AUC: 0.9855\n",
            "  Val Precision: 0.9266 | Val Recall: 0.9265\n",
            "âœ… Classifier saved to Google Drive: /content/drive/MyDrive/brain_mri_mae/models/classifier.pth\n",
            "  âœ… New best model saved! Val Acc: 0.9265\n",
            "--------------------------------------------------------------------------------\n",
            "=== GRADIENT CHECK ===\n",
            "pos_embed: grad norm = 0.240924\n",
            "cls_token: grad norm = 0.017579\n",
            "patch_embed.weight: grad norm = 7.389057\n",
            "patch_embed.bias: grad norm = 0.434816\n",
            "encoder_layers.layers.0.self_attn.in_proj_weight: grad norm = 2.094859\n",
            "encoder_layers.layers.0.self_attn.in_proj_bias: grad norm = 0.280060\n",
            "encoder_layers.layers.0.self_attn.out_proj.weight: grad norm = 2.527091\n",
            "encoder_layers.layers.0.self_attn.out_proj.bias: grad norm = 0.415150\n",
            "encoder_layers.layers.0.linear1.weight: grad norm = 1.233140\n",
            "encoder_layers.layers.0.linear1.bias: grad norm = 0.052715\n",
            "encoder_layers.layers.0.linear2.weight: grad norm = 2.037262\n",
            "encoder_layers.layers.0.linear2.bias: grad norm = 0.137041\n",
            "encoder_layers.layers.0.norm1.weight: grad norm = 0.128900\n",
            "encoder_layers.layers.0.norm1.bias: grad norm = 0.149941\n",
            "encoder_layers.layers.0.norm2.weight: grad norm = 0.130525\n",
            "encoder_layers.layers.0.norm2.bias: grad norm = 0.146603\n",
            "encoder_layers.layers.1.self_attn.in_proj_weight: grad norm = 2.220809\n",
            "encoder_layers.layers.1.self_attn.in_proj_bias: grad norm = 0.107526\n",
            "encoder_layers.layers.1.self_attn.out_proj.weight: grad norm = 1.514010\n",
            "encoder_layers.layers.1.self_attn.out_proj.bias: grad norm = 0.121213\n",
            "encoder_layers.layers.1.linear1.weight: grad norm = 0.893520\n",
            "encoder_layers.layers.1.linear1.bias: grad norm = 0.041128\n",
            "encoder_layers.layers.1.linear2.weight: grad norm = 1.512762\n",
            "encoder_layers.layers.1.linear2.bias: grad norm = 0.111552\n",
            "encoder_layers.layers.1.norm1.weight: grad norm = 0.093574\n",
            "encoder_layers.layers.1.norm1.bias: grad norm = 0.124494\n",
            "encoder_layers.layers.1.norm2.weight: grad norm = 0.099137\n",
            "encoder_layers.layers.1.norm2.bias: grad norm = 0.129797\n",
            "encoder_layers.layers.2.self_attn.in_proj_weight: grad norm = 1.818549\n",
            "encoder_layers.layers.2.self_attn.in_proj_bias: grad norm = 0.085992\n",
            "encoder_layers.layers.2.self_attn.out_proj.weight: grad norm = 1.298200\n",
            "encoder_layers.layers.2.self_attn.out_proj.bias: grad norm = 0.097541\n",
            "encoder_layers.layers.2.linear1.weight: grad norm = 0.767538\n",
            "encoder_layers.layers.2.linear1.bias: grad norm = 0.034633\n",
            "encoder_layers.layers.2.linear2.weight: grad norm = 1.208215\n",
            "encoder_layers.layers.2.linear2.bias: grad norm = 0.091051\n",
            "encoder_layers.layers.2.norm1.weight: grad norm = 0.079765\n",
            "encoder_layers.layers.2.norm1.bias: grad norm = 0.106681\n",
            "encoder_layers.layers.2.norm2.weight: grad norm = 0.082700\n",
            "encoder_layers.layers.2.norm2.bias: grad norm = 0.108995\n",
            "encoder_layers.layers.3.self_attn.in_proj_weight: grad norm = 1.672674\n",
            "encoder_layers.layers.3.self_attn.in_proj_bias: grad norm = 0.078101\n",
            "encoder_layers.layers.3.self_attn.out_proj.weight: grad norm = 0.807396\n",
            "encoder_layers.layers.3.self_attn.out_proj.bias: grad norm = 0.070540\n",
            "encoder_layers.layers.3.linear1.weight: grad norm = 0.500617\n",
            "encoder_layers.layers.3.linear1.bias: grad norm = 0.024024\n",
            "encoder_layers.layers.3.linear2.weight: grad norm = 0.752186\n",
            "encoder_layers.layers.3.linear2.bias: grad norm = 0.061493\n",
            "encoder_layers.layers.3.norm1.weight: grad norm = 0.053679\n",
            "encoder_layers.layers.3.norm1.bias: grad norm = 0.076557\n",
            "encoder_layers.layers.3.norm2.weight: grad norm = 0.052461\n",
            "encoder_layers.layers.3.norm2.bias: grad norm = 0.074055\n",
            "encoder_layers.layers.4.self_attn.in_proj_weight: grad norm = 0.976530\n",
            "encoder_layers.layers.4.self_attn.in_proj_bias: grad norm = 0.049719\n",
            "encoder_layers.layers.4.self_attn.out_proj.weight: grad norm = 0.522121\n",
            "encoder_layers.layers.4.self_attn.out_proj.bias: grad norm = 0.043787\n",
            "encoder_layers.layers.4.linear1.weight: grad norm = 0.301274\n",
            "encoder_layers.layers.4.linear1.bias: grad norm = 0.015130\n",
            "encoder_layers.layers.4.linear2.weight: grad norm = 0.433201\n",
            "encoder_layers.layers.4.linear2.bias: grad norm = 0.037620\n",
            "encoder_layers.layers.4.norm1.weight: grad norm = 0.033369\n",
            "encoder_layers.layers.4.norm1.bias: grad norm = 0.049764\n",
            "encoder_layers.layers.4.norm2.weight: grad norm = 0.031791\n",
            "encoder_layers.layers.4.norm2.bias: grad norm = 0.045416\n",
            "encoder_layers.layers.5.self_attn.in_proj_weight: grad norm = 0.537959\n",
            "encoder_layers.layers.5.self_attn.in_proj_bias: grad norm = 0.026927\n",
            "encoder_layers.layers.5.self_attn.out_proj.weight: grad norm = 0.394704\n",
            "encoder_layers.layers.5.self_attn.out_proj.bias: grad norm = 0.025951\n",
            "encoder_layers.layers.5.linear1.weight: grad norm = 0.213614\n",
            "encoder_layers.layers.5.linear1.bias: grad norm = 0.010174\n",
            "encoder_layers.layers.5.linear2.weight: grad norm = 0.271687\n",
            "encoder_layers.layers.5.linear2.bias: grad norm = 0.023852\n",
            "encoder_layers.layers.5.norm1.weight: grad norm = 0.024507\n",
            "encoder_layers.layers.5.norm1.bias: grad norm = 0.032900\n",
            "encoder_layers.layers.5.norm2.weight: grad norm = 0.022488\n",
            "encoder_layers.layers.5.norm2.bias: grad norm = 0.028636\n",
            "encoder_layers.layers.6.self_attn.in_proj_weight: grad norm = 0.336339\n",
            "encoder_layers.layers.6.self_attn.in_proj_bias: grad norm = 0.014960\n",
            "encoder_layers.layers.6.self_attn.out_proj.weight: grad norm = 0.321045\n",
            "encoder_layers.layers.6.self_attn.out_proj.bias: grad norm = 0.016803\n",
            "encoder_layers.layers.6.linear1.weight: grad norm = 0.182557\n",
            "encoder_layers.layers.6.linear1.bias: grad norm = 0.007657\n",
            "encoder_layers.layers.6.linear2.weight: grad norm = 0.199193\n",
            "encoder_layers.layers.6.linear2.bias: grad norm = 0.017089\n",
            "encoder_layers.layers.6.norm1.weight: grad norm = 0.020371\n",
            "encoder_layers.layers.6.norm1.bias: grad norm = 0.023609\n",
            "encoder_layers.layers.6.norm2.weight: grad norm = 0.017744\n",
            "encoder_layers.layers.6.norm2.bias: grad norm = 0.019846\n",
            "encoder_layers.layers.7.self_attn.in_proj_weight: grad norm = 0.223968\n",
            "encoder_layers.layers.7.self_attn.in_proj_bias: grad norm = 0.009275\n",
            "encoder_layers.layers.7.self_attn.out_proj.weight: grad norm = 0.248682\n",
            "encoder_layers.layers.7.self_attn.out_proj.bias: grad norm = 0.011876\n",
            "encoder_layers.layers.7.linear1.weight: grad norm = 0.135821\n",
            "encoder_layers.layers.7.linear1.bias: grad norm = 0.005385\n",
            "encoder_layers.layers.7.linear2.weight: grad norm = 0.151004\n",
            "encoder_layers.layers.7.linear2.bias: grad norm = 0.013445\n",
            "encoder_layers.layers.7.norm1.weight: grad norm = 0.016429\n",
            "encoder_layers.layers.7.norm1.bias: grad norm = 0.017516\n",
            "encoder_layers.layers.7.norm2.weight: grad norm = 0.014525\n",
            "encoder_layers.layers.7.norm2.bias: grad norm = 0.015358\n",
            "encoder_layers.layers.8.self_attn.in_proj_weight: grad norm = 0.180240\n",
            "encoder_layers.layers.8.self_attn.in_proj_bias: grad norm = 0.007033\n",
            "encoder_layers.layers.8.self_attn.out_proj.weight: grad norm = 0.207100\n",
            "encoder_layers.layers.8.self_attn.out_proj.bias: grad norm = 0.009343\n",
            "encoder_layers.layers.8.linear1.weight: grad norm = 0.110035\n",
            "encoder_layers.layers.8.linear1.bias: grad norm = 0.004209\n",
            "encoder_layers.layers.8.linear2.weight: grad norm = 0.130250\n",
            "encoder_layers.layers.8.linear2.bias: grad norm = 0.011702\n",
            "encoder_layers.layers.8.norm1.weight: grad norm = 0.013749\n",
            "encoder_layers.layers.8.norm1.bias: grad norm = 0.014465\n",
            "encoder_layers.layers.8.norm2.weight: grad norm = 0.012512\n",
            "encoder_layers.layers.8.norm2.bias: grad norm = 0.013152\n",
            "encoder_layers.layers.9.self_attn.in_proj_weight: grad norm = 0.162013\n",
            "encoder_layers.layers.9.self_attn.in_proj_bias: grad norm = 0.006139\n",
            "encoder_layers.layers.9.self_attn.out_proj.weight: grad norm = 0.183348\n",
            "encoder_layers.layers.9.self_attn.out_proj.bias: grad norm = 0.008089\n",
            "encoder_layers.layers.9.linear1.weight: grad norm = 0.095813\n",
            "encoder_layers.layers.9.linear1.bias: grad norm = 0.003595\n",
            "encoder_layers.layers.9.linear2.weight: grad norm = 0.126049\n",
            "encoder_layers.layers.9.linear2.bias: grad norm = 0.010674\n",
            "encoder_layers.layers.9.norm1.weight: grad norm = 0.012270\n",
            "encoder_layers.layers.9.norm1.bias: grad norm = 0.012863\n",
            "encoder_layers.layers.9.norm2.weight: grad norm = 0.011356\n",
            "encoder_layers.layers.9.norm2.bias: grad norm = 0.012081\n",
            "encoder_layers.layers.10.self_attn.in_proj_weight: grad norm = 0.157988\n",
            "encoder_layers.layers.10.self_attn.in_proj_bias: grad norm = 0.005900\n",
            "encoder_layers.layers.10.self_attn.out_proj.weight: grad norm = 0.171379\n",
            "encoder_layers.layers.10.self_attn.out_proj.bias: grad norm = 0.007614\n",
            "encoder_layers.layers.10.linear1.weight: grad norm = 0.091756\n",
            "encoder_layers.layers.10.linear1.bias: grad norm = 0.003396\n",
            "encoder_layers.layers.10.linear2.weight: grad norm = 0.135930\n",
            "encoder_layers.layers.10.linear2.bias: grad norm = 0.009968\n",
            "encoder_layers.layers.10.norm1.weight: grad norm = 0.010539\n",
            "encoder_layers.layers.10.norm1.bias: grad norm = 0.011781\n",
            "encoder_layers.layers.10.norm2.weight: grad norm = 0.010241\n",
            "encoder_layers.layers.10.norm2.bias: grad norm = 0.011498\n",
            "encoder_layers.layers.11.self_attn.in_proj_weight: grad norm = 0.163820\n",
            "encoder_layers.layers.11.self_attn.in_proj_bias: grad norm = 0.006081\n",
            "encoder_layers.layers.11.self_attn.out_proj.weight: grad norm = 0.167244\n",
            "encoder_layers.layers.11.self_attn.out_proj.bias: grad norm = 0.007838\n",
            "encoder_layers.layers.11.linear1.weight: grad norm = 0.101930\n",
            "encoder_layers.layers.11.linear1.bias: grad norm = 0.003723\n",
            "encoder_layers.layers.11.linear2.weight: grad norm = 0.164520\n",
            "encoder_layers.layers.11.linear2.bias: grad norm = 0.009901\n",
            "encoder_layers.layers.11.norm1.weight: grad norm = 0.009978\n",
            "encoder_layers.layers.11.norm1.bias: grad norm = 0.011663\n",
            "encoder_layers.layers.11.norm2.weight: grad norm = 0.010386\n",
            "encoder_layers.layers.11.norm2.bias: grad norm = 0.012217\n",
            "classifier.0.weight: grad norm = 0.011352\n",
            "classifier.0.bias: grad norm = 0.012872\n",
            "classifier.2.weight: grad norm = 0.214899\n",
            "classifier.2.bias: grad norm = 0.007741\n",
            "classifier.5.weight: grad norm = 1.898700\n",
            "classifier.5.bias: grad norm = 0.049122\n",
            "======================\n",
            "Batch 0/143, Loss: 0.0728\n",
            "Batch 50/143, Loss: 0.0076\n",
            "Batch 100/143, Loss: 0.0057\n",
            "Epoch 30/30 | Time: 180.61s\n",
            "  Train Loss: 0.0374 | Train Acc: 0.9928 | Train F1: 0.9928\n",
            "  Val Acc: 0.9256 | Val F1: 0.9256 | Val AUC: 0.9849\n",
            "  Val Precision: 0.9270 | Val Recall: 0.9256\n",
            "--------------------------------------------------------------------------------\n",
            "Training completed! Best validation accuracy: 0.9265\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION WITH BEST MODEL\n",
            "============================================================\n",
            "Accuracy:  0.9265\n",
            "F1 Score:  0.9264\n",
            "Precision: 0.9266\n",
            "Recall:    0.9265\n",
            "AUC-ROC:   0.9855\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      glioma       0.92      0.89      0.90       270\n",
            "  meningioma       0.87      0.88      0.87       263\n",
            "     notumor       0.97      0.96      0.97       319\n",
            "   pituitary       0.94      0.97      0.95       291\n",
            "\n",
            "    accuracy                           0.93      1143\n",
            "   macro avg       0.92      0.92      0.92      1143\n",
            "weighted avg       0.93      0.93      0.93      1143\n",
            "\n",
            "Figure(1500x1000)\n",
            "Figure(1000x800)\n",
            "Figure(800x600)\n",
            "âœ… All metrics and plots saved!\n"
          ]
        }
      ],
      "source": [
        "# RUN CLASSIFICATION FINE-TUNING\n",
        "%cd /content/brain_mri_mae\n",
        "!python run_finetune.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Classifier on Test Dataset"
      ],
      "metadata": {
        "id": "y0FmPwxwEAIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JAV7tqcVMb-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807ed5ae-25f0-43fd-c7e1-0a90c475a3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/brain_mri_mae\n",
            "Starting Classifier Testing...\n",
            "Using device: cuda\n",
            "Testing on dataset: dataset2\n",
            "[MRIDataset] Loaded 1311 samples from /kaggle/input/brain-tumor-mri-dataset/Testing\n",
            "[MRIDataset] Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "âœ… Test dataset: 1311 samples\n",
            "Number of classes: 4\n",
            "Class names: ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
            "Test samples: 1311\n",
            "âœ… Found classifier at: ./models/classifier.pth\n",
            "ðŸ”„ Loading classifier from: ./models/classifier.pth\n",
            "âœ… Classifier loaded successfully!\n",
            "\n",
            "============================================================\n",
            "COMPREHENSIVE TEST SET EVALUATION\n",
            "============================================================\n",
            "Test Loss:     0.2763\n",
            "Accuracy:      0.9436\n",
            "F1 Score:      0.9432\n",
            "Precision:     0.9441\n",
            "Recall:        0.9436\n",
            "AUC-ROC:       0.9866\n",
            "\n",
            "========================================\n",
            "DETAILED CLASSIFICATION REPORT\n",
            "========================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      glioma     0.9524    0.8667    0.9075       300\n",
            "  meningioma     0.8934    0.9314    0.9120       306\n",
            "     notumor     0.9755    0.9827    0.9791       405\n",
            "   pituitary     0.9453    0.9800    0.9624       300\n",
            "\n",
            "    accuracy                         0.9436      1311\n",
            "   macro avg     0.9417    0.9402    0.9402      1311\n",
            "weighted avg     0.9441    0.9436    0.9432      1311\n",
            "\n",
            "\n",
            "========================================\n",
            "PER-CLASS METRICS\n",
            "========================================\n",
            "glioma          | Precision: 0.9524 | Recall: 0.8667 | F1: 0.9075\n",
            "meningioma      | Precision: 0.8934 | Recall: 0.9314 | F1: 0.9120\n",
            "notumor         | Precision: 0.9755 | Recall: 0.9827 | F1: 0.9791\n",
            "pituitary       | Precision: 0.9453 | Recall: 0.9800 | F1: 0.9624\n",
            "\n",
            "ðŸ“Š Generating test set visualizations...\n",
            "Figure(1000x800)\n",
            "Figure(800x600)\n",
            "\n",
            "âœ… Test evaluation completed!\n",
            "ðŸ“ Results saved to:\n",
            "   - test_results.json\n",
            "   - test_roc_curves.png\n",
            "   - test_confusion_matrix.png\n",
            "\n",
            " FINAL TEST RESULTS:\n",
            "   Accuracy:  0.9436\n",
            "   F1 Score:  0.9432\n",
            "   AUC-ROC:   0.9866\n"
          ]
        }
      ],
      "source": [
        "%cd /content/brain_mri_mae\n",
        "\n",
        "!python test_classifier.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}